{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER 뉴스 데이터를 이용하여서 데이터 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup, Requests, Pandas 모듈 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터베이스 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스별 데이터베이스 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosun_eilbo_database = pd.read_csv(r'C:\\bigdata_git\\bigdata_thunder\\bigdata1\\chosun_eilbo.csv', encoding='cp949', index_col=False)\n",
    "korean_economy_database = pd.read_csv(r'C:\\bigdata_git\\bigdata_thunder\\bigdata1\\korean_economy.csv', encoding='utf-8', index_col=False)\n",
    "mail_economy_database = pd.read_csv(r'C:\\bigdata_git\\bigdata_thunder\\bigdata1\\mail_economy.csv', encoding='utf-8', index_col=False)\n",
    "midlle_eilbo_database = pd.read_csv(r'C:\\bigdata_git\\bigdata_thunder\\bigdata1\\middle_eilbo.csv', encoding='utf-8', index_col=False)\n",
    "money_today_database = pd.read_csv(r'C:\\bigdata_git\\bigdata_thunder\\bigdata1\\money_today.csv', encoding='utf-8', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "데이터베이스 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…</td>\n",
       "      <td>통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이준석 변호인 징계요청 기각… 경기변호사회 “수사결과 봐야”</td>\n",
       "      <td>“가세연의 이준석 녹취는 일부 삭제된 것” 경기중앙지방변호사회가 한 보수단체가 제기...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박지현 “이광재가 내 배후? 어리면 배후 있을 거라는 꼰대식 사고”</td>\n",
       "      <td>비대위원장직 자진사퇴 후 한 달 만에 공개행사 참석 이재명 전당대회 출마와 민형배 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>野 당권주자들 “검수완박 성급했다” 이제와서 반성 모드</td>\n",
       "      <td>강병원 “진영 논리서 벗어나야” 박용진 “상식 복원하는 게 혁신” 꼼수 탈당한 민형...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“먹으니 시야 흐려져” 시누이 부부 음식에 메탄올 넣은 30대여성</td>\n",
       "      <td>시누이 부부가 먹을 음식에 유독성 물질을 넣은 30대 여성이 특수상해 혐의로 경찰에...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13688</th>\n",
       "      <td>람보르기니 몰고 온 죄… 러 슈퍼카 차주들, 수갑 찬 채 끌려갔다</td>\n",
       "      <td>러시아에서 값비싼 슈퍼카를 몰던 차주들이 무더기로 경찰에 체포되는 일이 발생했다. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>10세 세계 최연소 트랜스젠더 모델…패션계가 주목</td>\n",
       "      <td>세계에서 가장 어린 트랜스젠더 모델인 미국 소녀 노엘라 맥마허(10)가 패션계의 주...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13690</th>\n",
       "      <td>[수요동물원] 살아있는 벌새의 뇌를… 소름 끼치는 사마귀의 먹방</td>\n",
       "      <td>곤충만 먹는줄 알았더니 개구리·도마뱀·새까지 먹어치워 짝짓기 중 동족포식하는 ‘이 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13691</th>\n",
       "      <td>[김한수의 오마이갓] 막 오른 ‘서울-로마 두 추기경 시대’</td>\n",
       "      <td>한국 천주교계에 ‘서울·로마 두 추기경 시대’의 막이 올랐습니다. 지난 27일(현지...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13692</th>\n",
       "      <td>[차현진의 돈과 세상] [86] 탕평책</td>\n",
       "      <td>베토벤이 청력을 잃어 불우했다고 하지만, 사실 인복은 많았다. 그는 커피를 끓이기 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13693 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0        판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…   \n",
       "1          이준석 변호인 징계요청 기각… 경기변호사회 “수사결과 봐야”   \n",
       "2      박지현 “이광재가 내 배후? 어리면 배후 있을 거라는 꼰대식 사고”   \n",
       "3             野 당권주자들 “검수완박 성급했다” 이제와서 반성 모드   \n",
       "4       “먹으니 시야 흐려져” 시누이 부부 음식에 메탄올 넣은 30대여성   \n",
       "...                                      ...   \n",
       "13688   람보르기니 몰고 온 죄… 러 슈퍼카 차주들, 수갑 찬 채 끌려갔다   \n",
       "13689            10세 세계 최연소 트랜스젠더 모델…패션계가 주목   \n",
       "13690    [수요동물원] 살아있는 벌새의 뇌를… 소름 끼치는 사마귀의 먹방   \n",
       "13691      [김한수의 오마이갓] 막 오른 ‘서울-로마 두 추기경 시대’   \n",
       "13692                  [차현진의 돈과 세상] [86] 탕평책   \n",
       "\n",
       "                                                 content  label  \\\n",
       "0      통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성...      0   \n",
       "1      “가세연의 이준석 녹취는 일부 삭제된 것” 경기중앙지방변호사회가 한 보수단체가 제기...      0   \n",
       "2      비대위원장직 자진사퇴 후 한 달 만에 공개행사 참석 이재명 전당대회 출마와 민형배 ...      0   \n",
       "3      강병원 “진영 논리서 벗어나야” 박용진 “상식 복원하는 게 혁신” 꼼수 탈당한 민형...      0   \n",
       "4      시누이 부부가 먹을 음식에 유독성 물질을 넣은 30대 여성이 특수상해 혐의로 경찰에...      0   \n",
       "...                                                  ...    ...   \n",
       "13688  러시아에서 값비싼 슈퍼카를 몰던 차주들이 무더기로 경찰에 체포되는 일이 발생했다. ...      0   \n",
       "13689  세계에서 가장 어린 트랜스젠더 모델인 미국 소녀 노엘라 맥마허(10)가 패션계의 주...      0   \n",
       "13690  곤충만 먹는줄 알았더니 개구리·도마뱀·새까지 먹어치워 짝짓기 중 동족포식하는 ‘이 ...      0   \n",
       "13691  한국 천주교계에 ‘서울·로마 두 추기경 시대’의 막이 올랐습니다. 지난 27일(현지...      0   \n",
       "13692  베토벤이 청력을 잃어 불우했다고 하지만, 사실 인복은 많았다. 그는 커피를 끓이기 ...      0   \n",
       "\n",
       "                                                     url  \n",
       "0      https://n.news.naver.com/mnews/article/023/000...  \n",
       "1      https://n.news.naver.com/mnews/article/023/000...  \n",
       "2      https://n.news.naver.com/mnews/article/023/000...  \n",
       "3      https://n.news.naver.com/mnews/article/023/000...  \n",
       "4      https://n.news.naver.com/mnews/article/023/000...  \n",
       "...                                                  ...  \n",
       "13688  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13689  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13690  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13691  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13692  https://n.news.naver.com/mnews/article/023/000...  \n",
       "\n",
       "[13693 rows x 4 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRT 탈선 구간 심야 복구…국토부 \"2일 오전 9시 정상화 예상\"</td>\n",
       "      <td>대전 조차장역 인근에서 수서행 SRT 열차 탈선 사고가 발생한 가운데 2일 오전 9...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'트윗 중독' 머스크, 열흘간 이례적 침묵…2017년 이후 처음</td>\n",
       "      <td>일론 머스크 테슬라 최고경영자(CEO)가 열흘 가까이 트윗을 중단했다. 1일(현지시...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이근 \"한국 고교생, 방학 맞아 우크라이나 들어와…미친 짓\"</td>\n",
       "      <td>우크라이나 국제의용군으로 합류한 뒤 귀국한 해군특수전전단(UDT/SEAL) 대위 출...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>美 델타항공 조종사들 뿔났다…\"파업 준비됐다\" 업무 과중 호소</td>\n",
       "      <td>미국 델타항공 조종사들이 처우개선을 요구하는 시위에 나섰다. 이들은 자신들의 요구가...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'가상화폐 여왕'의 몰락…5조 사기로 FBI 수배명단 올랐다</td>\n",
       "      <td>'가상화폐 여왕'이라는 별칭으로 알려진 국제사기범 루자 이그나토바가 미국 연방수사국...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26532</th>\n",
       "      <td>[사설] 윤석열 정부 긴축 의지 평가하지만, 대선공약 예산도 칼질해야</td>\n",
       "      <td>2023년도 정부 예산안이 모습을 드러냈다. 새 정부가 편성한 첫 예산인 데다 복합...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26533</th>\n",
       "      <td>[한경에세이] 독서예찬</td>\n",
       "      <td>얼마 전 지인이 취미를 묻길래 ‘독서, 음악 감상, 영화 감상’이라고 대답했더니, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26534</th>\n",
       "      <td>[사설] 명분도 실익도 없는 전기차 판매 목표제, 폐지해야 맞다</td>\n",
       "      <td>환경 분야에는 환경보호라는 명분에만 집착해 현실을 도외시한 정책이 적잖다. 전기차와...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26535</th>\n",
       "      <td>[사설] 취임 일성으로 '민생' 외친 이재명 대표…종부세 처리 앞장서라</td>\n",
       "      <td>이재명 더불어민주당 대표가 지난 28일 대표 당선 뒤부터 연일 강조하고 있는 게 민...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26536</th>\n",
       "      <td>[시론] 4차 산업혁명 이끄는 英 왕립예술대</td>\n",
       "      <td>1차 산업혁명 시기에 설립된 대학이 4차 산업혁명을 주도한다. 영국의 수도 런던에 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26537 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0        SRT 탈선 구간 심야 복구…국토부 \"2일 오전 9시 정상화 예상\"   \n",
       "1          '트윗 중독' 머스크, 열흘간 이례적 침묵…2017년 이후 처음   \n",
       "2            이근 \"한국 고교생, 방학 맞아 우크라이나 들어와…미친 짓\"   \n",
       "3           美 델타항공 조종사들 뿔났다…\"파업 준비됐다\" 업무 과중 호소   \n",
       "4            '가상화폐 여왕'의 몰락…5조 사기로 FBI 수배명단 올랐다   \n",
       "...                                        ...   \n",
       "26532   [사설] 윤석열 정부 긴축 의지 평가하지만, 대선공약 예산도 칼질해야   \n",
       "26533                             [한경에세이] 독서예찬   \n",
       "26534      [사설] 명분도 실익도 없는 전기차 판매 목표제, 폐지해야 맞다   \n",
       "26535  [사설] 취임 일성으로 '민생' 외친 이재명 대표…종부세 처리 앞장서라   \n",
       "26536                 [시론] 4차 산업혁명 이끄는 英 왕립예술대   \n",
       "\n",
       "                                                 content  label  \\\n",
       "0      대전 조차장역 인근에서 수서행 SRT 열차 탈선 사고가 발생한 가운데 2일 오전 9...      0   \n",
       "1      일론 머스크 테슬라 최고경영자(CEO)가 열흘 가까이 트윗을 중단했다. 1일(현지시...      0   \n",
       "2      우크라이나 국제의용군으로 합류한 뒤 귀국한 해군특수전전단(UDT/SEAL) 대위 출...      0   \n",
       "3      미국 델타항공 조종사들이 처우개선을 요구하는 시위에 나섰다. 이들은 자신들의 요구가...      0   \n",
       "4      '가상화폐 여왕'이라는 별칭으로 알려진 국제사기범 루자 이그나토바가 미국 연방수사국...      0   \n",
       "...                                                  ...    ...   \n",
       "26532  2023년도 정부 예산안이 모습을 드러냈다. 새 정부가 편성한 첫 예산인 데다 복합...      0   \n",
       "26533  얼마 전 지인이 취미를 묻길래 ‘독서, 음악 감상, 영화 감상’이라고 대답했더니, ...      0   \n",
       "26534  환경 분야에는 환경보호라는 명분에만 집착해 현실을 도외시한 정책이 적잖다. 전기차와...      0   \n",
       "26535  이재명 더불어민주당 대표가 지난 28일 대표 당선 뒤부터 연일 강조하고 있는 게 민...      0   \n",
       "26536  1차 산업혁명 시기에 설립된 대학이 4차 산업혁명을 주도한다. 영국의 수도 런던에 ...      0   \n",
       "\n",
       "                                                     url  \n",
       "0      https://n.news.naver.com/mnews/article/015/000...  \n",
       "1      https://n.news.naver.com/mnews/article/015/000...  \n",
       "2      https://n.news.naver.com/mnews/article/015/000...  \n",
       "3      https://n.news.naver.com/mnews/article/015/000...  \n",
       "4      https://n.news.naver.com/mnews/article/015/000...  \n",
       "...                                                  ...  \n",
       "26532  https://n.news.naver.com/mnews/article/015/000...  \n",
       "26533  https://n.news.naver.com/mnews/article/015/000...  \n",
       "26534  https://n.news.naver.com/mnews/article/015/000...  \n",
       "26535  https://n.news.naver.com/mnews/article/015/000...  \n",
       "26536  https://n.news.naver.com/mnews/article/015/000...  \n",
       "\n",
       "[26537 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_economy_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'문명특급' 수지 \"데뷔작 '드림하이' 당시 기억 거의 없어\"</td>\n",
       "      <td>'문명특급' 수지가 데뷔작 '드림하이'를 회상했다. 1일 방송된 SBS '문명특급'...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'나혼산' 김해준, 야성미 자랑 \"섹시한 돌쇠, 머슴같아\"</td>\n",
       "      <td>김해준의 야성미에 무지개 회원들이 감탄했다. 1일 밤 방송된 MBC '나 혼자 산다...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'문명특급' 재재, 수지와 함께 찍은 사진 공개 \"맞팔한 사이\" 너스레</td>\n",
       "      <td>'문명특급' 재재가 수지와 '맞팔' 사이라고 밝혔다. 1일 방송된 SBS '문명특급...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[종합] '닥터로이어' 소지섭, '父 죽음' 진실 알았다→ 신성록 \"소지섭, 병원장...</td>\n",
       "      <td>'닥터로이어' 소지섭이 아버지의 죽음에 대한 진실을 알았다. 1일 방송된 MBC 금...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[종합]'왜 오수재인가' 서현진, 허준호 판 흔들었다→황인엽과 동거·애정 확인</td>\n",
       "      <td>서현진이 이경영, 조영진, 허준호 등의 약점을 쥐면서 허준호가 짜놓은 판을 흔들었다...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27634</th>\n",
       "      <td>[사설] 통화긴축과 보조맞춘 내년 예산안 이제야 방향 바로 잡았다</td>\n",
       "      <td>정부가 내년 예산을 올해보다 5.2% 늘어난 639조원으로 편성했다. 5%대 본예산...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27635</th>\n",
       "      <td>윤석열·이재명 만나면 협치의 원칙 세워야 [사설]</td>\n",
       "      <td>윤석열 대통령과 이재명 더불어민주당 대표가 30일 통화를 하고 이른 시일 내에 만남...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27636</th>\n",
       "      <td>[사설] 수업중 교단에 드러눕고, 웃통 벗고…교권추락 상상초월</td>\n",
       "      <td>충남 홍성의 한 중학교에서 학생이 수업 중 교단에 벌렁 드러누워 휴대폰을 만지작거리...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27637</th>\n",
       "      <td>[오늘의 매일경제TV] 청년들 꿈을 키우는 목공소</td>\n",
       "      <td>■ 성공다큐 최고다 (31일 저녁 6시) 늘 그 자리에 있지만 사계절 흐름에 따라 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27638</th>\n",
       "      <td>[오늘의 MBN] 산에서 수련하는 퇴직 공무원</td>\n",
       "      <td>■ 나는 자연인이다 (31일 밤 9시 50분) 산에서 장풍을 수련하고 있다는 자연인...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27639 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0                     '문명특급' 수지 \"데뷔작 '드림하이' 당시 기억 거의 없어\"   \n",
       "1                       '나혼산' 김해준, 야성미 자랑 \"섹시한 돌쇠, 머슴같아\"   \n",
       "2                '문명특급' 재재, 수지와 함께 찍은 사진 공개 \"맞팔한 사이\" 너스레   \n",
       "3      [종합] '닥터로이어' 소지섭, '父 죽음' 진실 알았다→ 신성록 \"소지섭, 병원장...   \n",
       "4            [종합]'왜 오수재인가' 서현진, 허준호 판 흔들었다→황인엽과 동거·애정 확인   \n",
       "...                                                  ...   \n",
       "27634               [사설] 통화긴축과 보조맞춘 내년 예산안 이제야 방향 바로 잡았다   \n",
       "27635                        윤석열·이재명 만나면 협치의 원칙 세워야 [사설]   \n",
       "27636                 [사설] 수업중 교단에 드러눕고, 웃통 벗고…교권추락 상상초월   \n",
       "27637                        [오늘의 매일경제TV] 청년들 꿈을 키우는 목공소   \n",
       "27638                          [오늘의 MBN] 산에서 수련하는 퇴직 공무원   \n",
       "\n",
       "                                                 content  label  \\\n",
       "0      '문명특급' 수지가 데뷔작 '드림하이'를 회상했다. 1일 방송된 SBS '문명특급'...      0   \n",
       "1      김해준의 야성미에 무지개 회원들이 감탄했다. 1일 밤 방송된 MBC '나 혼자 산다...      0   \n",
       "2      '문명특급' 재재가 수지와 '맞팔' 사이라고 밝혔다. 1일 방송된 SBS '문명특급...      0   \n",
       "3      '닥터로이어' 소지섭이 아버지의 죽음에 대한 진실을 알았다. 1일 방송된 MBC 금...      0   \n",
       "4      서현진이 이경영, 조영진, 허준호 등의 약점을 쥐면서 허준호가 짜놓은 판을 흔들었다...      0   \n",
       "...                                                  ...    ...   \n",
       "27634  정부가 내년 예산을 올해보다 5.2% 늘어난 639조원으로 편성했다. 5%대 본예산...      0   \n",
       "27635  윤석열 대통령과 이재명 더불어민주당 대표가 30일 통화를 하고 이른 시일 내에 만남...      0   \n",
       "27636  충남 홍성의 한 중학교에서 학생이 수업 중 교단에 벌렁 드러누워 휴대폰을 만지작거리...      0   \n",
       "27637  ■ 성공다큐 최고다 (31일 저녁 6시) 늘 그 자리에 있지만 사계절 흐름에 따라 ...      0   \n",
       "27638  ■ 나는 자연인이다 (31일 밤 9시 50분) 산에서 장풍을 수련하고 있다는 자연인...      0   \n",
       "\n",
       "                                                     url  \n",
       "0      https://n.news.naver.com/mnews/article/009/000...  \n",
       "1      https://n.news.naver.com/mnews/article/009/000...  \n",
       "2      https://n.news.naver.com/mnews/article/009/000...  \n",
       "3      https://n.news.naver.com/mnews/article/009/000...  \n",
       "4      https://n.news.naver.com/mnews/article/009/000...  \n",
       "...                                                  ...  \n",
       "27634  https://n.news.naver.com/mnews/article/009/000...  \n",
       "27635  https://n.news.naver.com/mnews/article/009/000...  \n",
       "27636  https://n.news.naver.com/mnews/article/009/000...  \n",
       "27637  https://n.news.naver.com/mnews/article/009/000...  \n",
       "27638  https://n.news.naver.com/mnews/article/009/000...  \n",
       "\n",
       "[27639 rows x 4 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail_economy_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유나양 마지막길, 유족은 오지 않았다…장례식 없이 화장</td>\n",
       "      <td>전남 완도 바다에서 숨진 채 발견된 조유나(10)양과 부모가 유족 없이 1일 화장됐...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>한화전에서 역전 3타점 3루타를 터뜨려 승리를 이끈 키움 김준완. [연합뉴스] 프로...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박지원 \"대선·지선 승리 이끌고 손절 당해…이준석 안타깝기도\"</td>\n",
       "      <td>박지원 전 국정원장은 이준석 국민의힘 대표가 해외순방에서 돌아온 윤석열 대통령을 마...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[그림사설] 검수완박 논란, 헌재가 빨리 답 내놔야</td>\n",
       "      <td>글=중앙일보 논설실 그림=</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>러 \"러-독 연결 가스관 10여일 간 잠정폐쇄…기술 점검 차원\"</td>\n",
       "      <td>러시아가 발트해를 통해 독일로 연결되는 '노르트 스트림' 가스관을 이달 중순 10여...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14440</th>\n",
       "      <td>떠돌던 충정공 민영환 동상, 충정로에 자리 잡았다</td>\n",
       "      <td>“나라의 치욕과 백성의 욕됨이 여기에 이르렀으니, 우리 민족은 장차 생존 경쟁에서 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14441</th>\n",
       "      <td>대법 “긴급조치 9호 피해자 국가배상”…7년 전 판례 뒤집어</td>\n",
       "      <td>박정희 전 대통령의 ‘긴급조치 9호’가 헌법에 어긋날 뿐만 아니라, 유신정권 하에서...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>초등 1·2학년 국어수업 34시간 늘려 문해력 키운다</td>\n",
       "      <td>2024년부터 초등학교 1·2학년의 국어 수업 시수가 늘어난다. 또 전 학년 모든 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>올해 가장 센 태풍 힌남노, 금요일쯤 한반도로 방향 튼다</td>\n",
       "      <td>제11호 태풍 ‘힌남노(HINNAMNOR)’가 초강력 태풍으로 성장하며 서쪽으로 이...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>권성동 “비대위 구성 뒤 거취 표명”…선수습 후사퇴 가닥</td>\n",
       "      <td>국민의힘이 30일 의원총회에서 ‘비상 상황’을 구체적으로 규정하는 당헌 개정을 추진...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14445 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "0           유나양 마지막길, 유족은 오지 않았다…장례식 없이 화장   \n",
       "1                                      NaN   \n",
       "2       박지원 \"대선·지선 승리 이끌고 손절 당해…이준석 안타깝기도\"   \n",
       "3             [그림사설] 검수완박 논란, 헌재가 빨리 답 내놔야   \n",
       "4      러 \"러-독 연결 가스관 10여일 간 잠정폐쇄…기술 점검 차원\"   \n",
       "...                                    ...   \n",
       "14440          떠돌던 충정공 민영환 동상, 충정로에 자리 잡았다   \n",
       "14441    대법 “긴급조치 9호 피해자 국가배상”…7년 전 판례 뒤집어   \n",
       "14442        초등 1·2학년 국어수업 34시간 늘려 문해력 키운다   \n",
       "14443      올해 가장 센 태풍 힌남노, 금요일쯤 한반도로 방향 튼다   \n",
       "14444      권성동 “비대위 구성 뒤 거취 표명”…선수습 후사퇴 가닥   \n",
       "\n",
       "                                                 content  label  \\\n",
       "0      전남 완도 바다에서 숨진 채 발견된 조유나(10)양과 부모가 유족 없이 1일 화장됐...      0   \n",
       "1      한화전에서 역전 3타점 3루타를 터뜨려 승리를 이끈 키움 김준완. [연합뉴스] 프로...      0   \n",
       "2      박지원 전 국정원장은 이준석 국민의힘 대표가 해외순방에서 돌아온 윤석열 대통령을 마...      0   \n",
       "3                                         글=중앙일보 논설실 그림=      0   \n",
       "4      러시아가 발트해를 통해 독일로 연결되는 '노르트 스트림' 가스관을 이달 중순 10여...      0   \n",
       "...                                                  ...    ...   \n",
       "14440  “나라의 치욕과 백성의 욕됨이 여기에 이르렀으니, 우리 민족은 장차 생존 경쟁에서 ...      0   \n",
       "14441  박정희 전 대통령의 ‘긴급조치 9호’가 헌법에 어긋날 뿐만 아니라, 유신정권 하에서...      0   \n",
       "14442  2024년부터 초등학교 1·2학년의 국어 수업 시수가 늘어난다. 또 전 학년 모든 ...      0   \n",
       "14443  제11호 태풍 ‘힌남노(HINNAMNOR)’가 초강력 태풍으로 성장하며 서쪽으로 이...      0   \n",
       "14444  국민의힘이 30일 의원총회에서 ‘비상 상황’을 구체적으로 규정하는 당헌 개정을 추진...      0   \n",
       "\n",
       "                                                     url  \n",
       "0      https://n.news.naver.com/mnews/article/025/000...  \n",
       "1      https://n.news.naver.com/mnews/article/025/000...  \n",
       "2      https://n.news.naver.com/mnews/article/025/000...  \n",
       "3      https://n.news.naver.com/mnews/article/025/000...  \n",
       "4      https://n.news.naver.com/mnews/article/025/000...  \n",
       "...                                                  ...  \n",
       "14440  https://n.news.naver.com/mnews/article/025/000...  \n",
       "14441  https://n.news.naver.com/mnews/article/025/000...  \n",
       "14442  https://n.news.naver.com/mnews/article/025/000...  \n",
       "14443  https://n.news.naver.com/mnews/article/025/000...  \n",
       "14444  https://n.news.naver.com/mnews/article/025/000...  \n",
       "\n",
       "[14445 rows x 4 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midlle_eilbo_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_today_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표본별 뉴스 제목중 하나 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 출처 홈페이지 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://n.news.naver.com/mnews/article/023/0003701167'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database['url'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깃허브 오픈소스 이용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화로 바꾸는 파일 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ git clone https://github.com/ukairia777/tensorflow-nlp-tutorial.git\n",
    "# $ git clone https://github.com/haven-jeon/PyKoSpacing.git\n",
    "# $ git clone https://github.com/ssut/py-hanspell.git\n",
    "# %pip install soynlp\n",
    "# %pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python38\\lib\\site-packages (2.10.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python38\\lib\\site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\python38\\lib\\site-packages (from tensorflow) (64.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\python38\\lib\\site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: packaging in c:\\python38\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python38\\lib\\site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python38\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python38\\lib\\site-packages (from tensorflow) (1.23.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python38\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python38\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\python38\\lib\\site-packages (3.6.4)\n",
      "Requirement already satisfied: regex in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from kss) (2022.9.13)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\python38\\lib\\site-packages (from kss) (1.2.0)\n",
      "Requirement already satisfied: more-itertools in c:\\python38\\lib\\site-packages (from kss) (8.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\python38\\lib\\site-packages (from konlpy) (1.23.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\python38\\lib\\site-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\python38\\lib\\site-packages (from konlpy) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soynlp in c:\\python38\\lib\\site-packages (0.0.493)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\python38\\lib\\site-packages (from soynlp) (5.9.1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\python38\\lib\\site-packages (from soynlp) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\python38\\lib\\site-packages (from soynlp) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\python38\\lib\\site-packages (from soynlp) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python38\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: customized_konlpy in c:\\python38\\lib\\site-packages (0.0.64)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\python38\\lib\\site-packages (from customized_konlpy) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\python38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (1.23.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\python38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (4.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n",
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n",
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n",
      "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "19  1990  52 1 22\n",
      "오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
      "테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n",
      "training was done. used memory 0.854 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\AppData\\Roaming\\Python\\Python38\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing_tools_for_korean_text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url 홈페이지 BeautifulSoup을이용해서 소스에 있는 내용 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다. /픽사베이통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성이 최소 600만년 전부터 이어졌을 가능성이 있다는 연구 결과가 나왔다.1일(현지 시각) CNN 등 외신에 따르면 미국 로스앤젤레스 자연사 박물관의 왕 샤오밍 박사 연구팀은 최근 약 600만년 전 자이언트 판다 조상인 아일루락토스(Ailurarctos) 화석을 분석한 결과, 대나무를 잡는 데 쓰이는 ‘가짜 엄지’가 존재했다고 밝혔다.이 화석은 중국 남부 윈난성 자오퉁시의 슈이탕바 지역에서 발굴됐다. 약 600만년 전 살았던 고대 판다 아일루락토스의 것이다. 눈여겨볼 것은 손목 부위에서 발견된 돌출 뼈다. 연구팀은 바로 이 뼈가 자이언트 판다가 가진 여섯 번째 손가락 일명 ‘가짜 엄지’라고 말했다.판다의 가짜 엄지. /뉴시스판다의 가짜 엄지는 대나무를 보다 쉽게 잡을 수 있도록 하는 데 쓰인다. 판다가 초식동물로 진화하는 데 중추적인 역할을 했을 것으로 전문가들은 보고 있다. 그동안 이같은 진화적 적응은 약 15만년 전인 비교적 최근 이뤄진 것으로 여겨져 왔는데, 이번 연구가 무려 600만년을 거슬러 올라간다는 것을 증명해낸 셈이다.화석에서 발견된 가짜 엄지는 현대 판다의 것보다 더 긴 길이의 직선 모양을 하고 있었다. 연구팀은 “가짜 엄지는 대나무를 잡고 뜯어먹을 때는 물론 다음 먹이를 찾아 걸어갈 때 몸무게를 지탱하는 용도로도 쓰이는데, 이 과정에서 긴 뼈가 짧은 갈고리형으로 진화하게 된 것”이라고 분석했다.왕 박사는 “대나무를 먹기 좋게 쪼개기 위해 줄기를 단단히 붙잡는 것은 많은 양의 대나무를 먹는 데 가장 중요한 적응”이라며 “육식성 조상에서 진화해 대나무만 먹는 종으로 바뀐 판다는 많은 장애를 넘어야 했을 것이고 가짜 엄지는 그중 가장 놀라운 진화”라고 설명했다.\n",
      "1/1 [==============================] - 1s 895ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "판다. / 픽사베이통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성이 최소 600만년 전부터 이어졌을 가능성이 있다는 연구 결과가 나왔다.1일(현지시각) CNN 등 외신에 따르면 미국 로스앤젤레스 자연사박물관의 왕샤오밍 박사 연구팀은 최근 약 600만년 전자이언트 판다 조상인 아일루락토스(A ilurarctos) 화석을 분석한 결과, 대나무를 잡는 데 쓰이는 ‘가짜 엄지’가 존재했다고 밝혔다.이 화석은 중국 남부 윈 난성자오퉁시의 슈이 탕바지역에서 발굴됐다. 약 600만년 전 살았던 고대 판다 아일루락토스의 것이다. 눈여겨볼 것은 손목 부위에서 발견된 돌출뼈다. 연구팀은 바로 이 뼈가 자이언트 판다가 가진 여섯 번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다의 가짜 엄지. / 뉴시스판다의 가짜 엄지는 대나무를 보다 쉽게 잡을 수 있도록 하는데 쓰인다. 판다가 초식동물로 진화하는 데 중추적인 역할을 했을 것으로 전문가들은 보고 있다. 그동안 이 같은 진화적 적응은 약 15만년 전 인 비교적 최근 이뤄진 것으로 여겨져 왔는데, 이번 연구가 무려 600만년을 거슬러 올라간다는 것을 증명해낸 셈이다. 화석에서 발견된 가짜 엄지는 현대판다의 것보다 더 긴 길이의 직선 모양을 하고 있었다. 연구팀은 “가짜 엄지는 대나무를 잡고 뜯어먹을 때는 물론 다음 먹이를 찾아 걸어갈 때 몸무게를 지탱하는 용도로도 쓰이는데, 이 과정에서 긴 뼈가 짧은 갈고리형으로 진화하게 된 것”이라고 분석했다. 왕 박사는 “대나무를 먹기 좋게 쪼개기 위해 줄기를 단단히 붙잡는 것은 많은 양의 대나무를 먹는 데 가장 중요한 적응”이라며 “육식성 조상에서 진화해 대나무만 먹는 종으로 바뀐 판다는 많은 장애를 넘어야 했을 것이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.\n"
     ]
    }
   ],
   "source": [
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"}\n",
    "response_0 = requests.get(chosun_eilbo_database[\"url\"][0], headers=headers)\n",
    "soup_0 = BeautifulSoup(response_0.text, 'html.parser')\n",
    "content_0_data = soup_0.find('div', {\"id\" : \"dic_area\"}).get_text()\n",
    "content_0_data = content_0_data.replace('\\n','').replace('\\t','')\n",
    "print(content_0_data)\n",
    "new_content_0_data = content_0_data.replace(\" \", '')\n",
    "\n",
    "kospacing_new_content_0_data = spacing(new_content_0_data) \n",
    "print(kospacing_new_content_0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '과', '어울', '리지', '않게', '대나무', '를', '주식', '으로', '하는', '판다', '.', '이런', '판다', '의', '독특', '한', '식성', '이', '최소', '600', '만년', '전부', '터', '이어졌을', '가능성', '이', '있다는', '연구', '결과', '가', '나왔다', '.', '1일', '(', '현지', '시각', ')', 'CNN', '등', '외신', '에', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '의', '왕샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '아', '일루', '락토스', '(', 'A', 'ilurarctos', ')', '화석', '을', '분석', '한', '결과', ',', '대나무', '를', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '가', '존재', '했다고', '밝혔다', '.', '이', '화석', '은', '중국', '남부', '윈', '난', '성', '자오퉁시', '의', '슈이', '탕바', '지역', '에서', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '아', '일루', '락토스', '의', '것', '이다', '.', '눈', '여겨', '볼', '것', '은', '손목', '부위', '에서', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '바로', '이', '뼈', '가', '자이언트', '판다', '가', '가진', '여섯', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '의', '가짜', '엄지', '.', '/', '뉴시스', '판다', '의', '가짜', '엄지', '는', '대나무', '를', '보다', '쉽게', '잡', '을', '수', '있', '도록', '하는데', '쓰인다', '.', '판다', '가', '초식동물', '로', '진화', '하는', '데', '중', '추적', '인', '역할', '을', '했을', '것', '으로', '전문가', '들은', '보고', '있다', '.', '그동안', '이', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전', '인', '비교', '적', '최근', '이뤄진', '것', '으로', '여겨져', '왔는데', ',', '이번', '연구', '가', '무려', '600', '만년', '을', '거슬러', '올라', '간다', '는', '것', '을', '증명', '해', '낸', '셈', '이다', '.', '화석', '에서', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '의', '것', '보다', '더', '긴', '길이', '의', '직선', '모양', '을', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '를', '잡고', '뜯어', '먹을', '때', '는', '물론', '다음', '먹이', '를', '찾아', '걸어갈', '때', '몸무게', '를', '지탱', '하는', '용도', '로', '도', '쓰이는데', ',', '이', '과정', '에서', '긴', '뼈', '가', '짧은', '갈고리', '형', '으로', '진화', '하게', '된', '것', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '를', '먹기', '좋게', '쪼개기', '위해', '줄기', '를', '단단히', '붙잡는', '것', '은', '많은', '양', '의', '대나무', '를', '먹는', '데', '가장', '중요', '한', '적응', '”', '이', '라며', '“', '육식성', '조상', '에서', '진화', '해', '대나무', '만', '먹는', '종', '으로', '바뀐', '판다', '는', '많은', '장애', '를', '넘어야', '했을', '것', '이고', '가짜', '엄지', '는', '그', '중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs(kospacing_new_content_0_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = twitter.morphs(content_0_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 제거하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
      "die\n",
      "watch\n",
      "have\n",
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n",
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from stemming_lemmatization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for word in words_data:\n",
    "#     print(lemmatizer.lemmatize(word))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 내용항목 중 한국어 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from remove_stopwords import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '과', '어울', '리지', '않게', '대나무', '를', '주식', '으로', '하는', '판다', '.', '이런', '판다', '의', '독특', '한', '식성', '이', '최소', '600', '만년', '전부', '터', '이어졌을', '가능성', '이', '있다는', '연구', '결과', '가', '나왔다', '.', '1일', '(', '현지', '시각', ')', 'CNN', '등', '외신', '에', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '의', '왕', '샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '아', '일루', '락토스', '(', 'Ailurarctos', ')', '화석', '을', '분석', '한', '결과', ',', '대나무', '를', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '가', '존재', '했다고', '밝혔다', '.', '이', '화석', '은', '중국', '남부', '윈난성', '자오퉁시', '의', '슈이탕바', '지역', '에서', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '아', '일루', '락토스', '의', '것', '이다', '.', '눈', '여겨', '볼', '것', '은', '손목', '부위', '에서', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '바로', '이', '뼈', '가', '자이언트', '판다', '가', '가진', '여섯', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '의', '가짜', '엄지', '.', '/', '뉴시스', '판다', '의', '가짜', '엄지', '는', '대나무', '를', '보다', '쉽게', '잡', '을', '수', '있', '도록', '하는', '데', '쓰인다', '.', '판다', '가', '초식동물', '로', '진화', '하는', '데', '중', '추적', '인', '역할', '을', '했을', '것', '으로', '전문가', '들은', '보고', '있다', '.', '그동안', '이', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전인', '비교', '적', '최근', '이뤄진', '것', '으로', '여겨져', '왔는데', ',', '이번', '연구', '가', '무려', '600', '만년', '을', '거슬러', '올라', '간다', '는', '것', '을', '증명', '해', '낸', '셈', '이다', '.', '화석', '에서', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '의', '것', '보다', '더', '긴', '길이', '의', '직선', '모양', '을', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '를', '잡고', '뜯어', '먹을', '때', '는', '물론', '다음', '먹이', '를', '찾아', '걸어갈', '때', '몸무게', '를', '지탱', '하는', '용도', '로', '도', '쓰이는데', ',', '이', '과정', '에서', '긴', '뼈', '가', '짧은', '갈고리', '형', '으로', '진화', '하게', '된', '것', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '를', '먹기', '좋게', '쪼개기', '위해', '줄기', '를', '단단히', '붙잡는', '것', '은', '많은', '양', '의', '대나무', '를', '먹는', '데', '가장', '중요', '한', '적응', '”', '이', '라며', '“', '육식성', '조상', '에서', '진화', '해', '대나무', '만', '먹는', '종', '으로', '바뀐', '판다', '는', '많은', '장애', '를', '넘어야', '했을', '것', '이고', '가짜', '엄지', '는', '그중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n",
      "\n",
      "불용어 제거 후 : ['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '어울', '리지', '않게', '대나무', '주식', '하는', '판다', '.', '판다', '독특', '한', '식성', '최소', '600', '만년', '터', '이어졌을', '가능성', '있다는', '연구', '결과', '나왔다', '.', '1일', '(', '현지', ')', 'CNN', '외신', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '왕', '샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '일루', '락토스', '(', 'Ailurarctos', ')', '화석', '분석', '한', '결과', ',', '대나무', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '존재', '했다고', '밝혔다', '.', '화석', '은', '중국', '남부', '윈난성', '자오퉁시', '슈이탕바', '지역', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '일루', '락토스', '이다', '.', '눈', '여겨', '볼', '은', '손목', '부위', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '뼈', '자이언트', '판다', '가진', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '가짜', '엄지', '.', '/', '뉴시스', '판다', '가짜', '엄지', '는', '대나무', '보다', '쉽게', '잡', '수', '있', '도록', '하는', '데', '쓰인다', '.', '판다', '초식동물', '진화', '하는', '데', '중', '추적', '인', '역할', '했을', '전문가', '들은', '보고', '.', '그동안', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전인', '비교', '적', '최근', '이뤄진', '여겨져', '왔는데', ',', '연구', '무려', '600', '만년', '거슬러', '올라', '간다', '는', '증명', '해', '낸', '셈', '이다', '.', '화석', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '보다', '더', '긴', '길이', '직선', '모양', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '잡고', '뜯어', '먹을', '는', '먹이', '찾아', '걸어갈', '몸무게', '지탱', '하는', '용도', '도', '쓰이는데', ',', '과정', '긴', '뼈', '짧은', '갈고리', '형', '진화', '하게', '된', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '먹기', '좋게', '쪼개기', '위해', '줄기', '단단히', '붙잡는', '은', '많은', '양', '대나무', '먹는', '데', '가장', '중요', '한', '적응', '”', '라며', '“', '육식성', '조상', '진화', '해', '대나무', '만', '먹는', '종', '바뀐', '판다', '는', '많은', '장애', '넘어야', '했을', '이고', '가짜', '엄지', '는', '그중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:\\\\bigdata_git\\\\bigdata_thunder\\\\bigdata2\\\\korean_stopwords.txt\", \"r\", encoding=\"UTF-8\") as stopwords:\n",
    "    stopwords_list = stopwords.read().split('\\n')\n",
    "print('불용어 제거 전 :',words_data) \n",
    "print()\n",
    "print('불용어 제거 후 :',korean_change_no_stop_words(words_data, stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다./픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소600만년터이어졌을가능성있다는연구결과나왔다.1일(현지)CNN외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약600만년전자이언트판다조상인일루락토스(Ailurarctos)화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약600만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지./뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약15만년전인비교적최근이뤄진여겨져왔는데,연구무려600만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.\n"
     ]
    }
   ],
   "source": [
    "new_content_1 ='' \n",
    "for i in korean_change_no_stop_words(words_data, stopwords_list):\n",
    "    new_content_1 += i\n",
    "print(new_content_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 텍스트 정규화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n",
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from regular_expression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다.', '픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소', '만년터이어졌을가능성있다는연구결과나왔다.', '일', '현지', '외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약', '만년전자이언트판다조상인일루락토스', '화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약', '만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지.', '뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약', '만년전인비교적최근이뤄진여겨져왔는데,연구무려', '만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer3_content_1 = RegexpTokenizer('[A-Za-z0-9ㄱ-ㅎ/()]+', gaps=True)\n",
    "# tokenizer3_content_2 = RegexpTokenizer('')\n",
    "print(tokenizer3_content_1.tokenize(new_content_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다.픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소만년터이어졌을가능성있다는연구결과나왔다.일현지외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약만년전자이언트판다조상인일루락토스화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지.뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약만년전인비교적최근이뤄진여겨져왔는데,연구무려만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.\n"
     ]
    }
   ],
   "source": [
    "regular_expression_sentence = ''\n",
    "for expression in tokenizer3_content_1.tokenize(new_content_1):\n",
    "    regular_expression_sentence += expression\n",
    "print(regular_expression_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n",
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n",
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n",
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n",
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n",
      "단어 OOV의 인덱스 : 1\n",
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from integer_encoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\bigdata_git\\bigdata_thunder\\bigdata2\\practice_bigdata_project.ipynb 셀 43\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vocab \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m preprocessed_sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sent_tokenize(regular_expression_sentence):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# 단어 토큰화\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokenized_sentence \u001b[39m=\u001b[39m twitter\u001b[39m.\u001b[39mnouns(sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokenized_sentence: \n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for sentence in sent_tokenize(regular_expression_sentence):\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = twitter.nouns(sentence)\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        if len(word) > 2: \n",
    "            result.append(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0 \n",
    "            vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대나무': 7, '가능성': 1, '로스앤젤레스': 1, '자연사': 1, '박물관': 1, '왕샤오밍': 1, '자이언트판다': 2, '락토스': 2, '윈난성': 1, '자오퉁시': 1, '슈이탕바': 1, '손가락': 1, '뉴시스': 1, '초식동물': 1, '전문가': 1, '그동안': 1, '년거슬러올': 1, '몸무게': 1, '갈고리': 1, '육식성': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'went', 'huge', 'mountain', '대나무', '가능성', '로스앤젤레스', '자연사', '박물관', '왕샤오밍', '자이언트판다', '락토스', '대나무', '윈난성', '자오퉁시', '슈이탕바', '락토스', '자이언트판다', '손가락', '뉴시스', '대나무', '초식동물', '전문가', '그동안', '년거슬러올', '대나무', '몸무게', '갈고리', '대나무', '대나무', '육식성', '대나무']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'대나무': 7, '자이언트판다': 2, '락토스': 2, 'barber': 1, 'went': 1, 'huge': 1, 'mountain': 1, '가능성': 1, '로스앤젤레스': 1, '자연사': 1, '박물관': 1, '왕샤오밍': 1, '윈난성': 1, '자오퉁시': 1, '슈이탕바': 1, '손가락': 1, '뉴시스': 1, '초식동물': 1, '전문가': 1, '그동안': 1, '년거슬러올': 1, '몸무게': 1, '갈고리': 1, '육식성': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"판다\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('대나무', 7), ('자이언트판다', 2), ('락토스', 2), ('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('가능성', 1), ('로스앤젤레스', 1), ('자연사', 1)]\n",
      "{'대나무': 1, '자이언트판다': 2, '락토스': 3, 'barber': 4, 'went': 5, 'huge': 6, 'mountain': 7, '가능성': 8, '로스앤젤레스': 9, '자연사': 10}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)\n",
    "\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대나무': 1, '자이언트판다': 2, '락토스': 3, 'barber': 4, 'went': 5, 'huge': 6, 'mountain': 7, '가능성': 8, '로스앤젤레스': 9, '자연사': 10, '박물관': 11, '왕샤오밍': 12, '윈난성': 13, '자오퉁시': 14, '슈이탕바': 15, '손가락': 16, '뉴시스': 17, '초식동물': 18, '전문가': 19, '그동안': 20, '년거슬러올': 21, '몸무게': 22, '갈고리': 23, '육식성': 24}\n",
      "\n",
      "OrderedDict([('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('대나무', 7), ('가능성', 1), ('로스앤젤레스', 1), ('자연사', 1), ('박물관', 1), ('왕샤오밍', 1), ('자이언트판다', 2), ('락토스', 2), ('윈난성', 1), ('자오퉁시', 1), ('슈이탕바', 1), ('손가락', 1), ('뉴시스', 1), ('초식동물', 1), ('전문가', 1), ('그동안', 1), ('년거슬러올', 1), ('몸무게', 1), ('갈고리', 1), ('육식성', 1)])\n",
      "\n",
      "[[4, 5, 6, 7, 1, 8, 9, 10, 11, 12, 2, 3, 1, 13, 14, 15, 3, 2, 16, 17, 1, 18, 19, 20, 21, 1, 22, 23, 1, 1, 24, 1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대나무': 1, '자이언트판다': 2, '락토스': 3, 'barber': 4, 'went': 5, 'huge': 6, 'mountain': 7, '가능성': 8, '로스앤젤레스': 9, '자연사': 10, '박물관': 11, '왕샤오밍': 12, '윈난성': 13, '자오퉁시': 14, '슈이탕바': 15, '손가락': 16, '뉴시스': 17, '초식동물': 18, '전문가': 19, '그동안': 20, '년거슬러올': 21, '몸무게': 22, '갈고리': 23, '육식성': 24}\n",
      "\n",
      "OrderedDict([('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('대나무', 7), ('가능성', 1), ('로스앤젤레스', 1), ('자연사', 1), ('박물관', 1), ('왕샤오밍', 1), ('자이언트판다', 2), ('락토스', 2), ('윈난성', 1), ('자오퉁시', 1), ('슈이탕바', 1), ('손가락', 1), ('뉴시스', 1), ('초식동물', 1), ('전문가', 1), ('그동안', 1), ('년거슬러올', 1), ('몸무게', 1), ('갈고리', 1), ('육식성', 1)])\n",
      "\n",
      "[[4, 5, 6, 7, 1, 8, 9, 10, 2, 3, 1, 3, 2, 1, 1, 1, 1, 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 10개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
    "print()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대나무': 1, '자이언트판다': 2, '락토스': 3, 'barber': 4, 'went': 5, 'huge': 6, 'mountain': 7, '가능성': 8, '로스앤젤레스': 9, '자연사': 10}\n",
      "OrderedDict([('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('대나무', 7), ('가능성', 1), ('로스앤젤레스', 1), ('자연사', 1), ('자이언트판다', 2), ('락토스', 2)])\n",
      "[[4, 5, 6, 7, 1, 8, 9, 10, 2, 3, 1, 3, 2, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for word in words_frequency:\n",
    "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python38\\lib\\site-packages (2.10.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\python38\\lib\\site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\python38\\lib\\site-packages (from tensorflow) (64.0.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python38\\lib\\site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: packaging in c:\\python38\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python38\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python38\\lib\\site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python38\\lib\\site-packages (from tensorflow) (1.23.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python38\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\python38\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python38\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python38\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82105\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\python38\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python38\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n",
      "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from one_encoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '는', '언제', '부터', '대나무', '만', '먹었을까', '?', '600', '만년', '전', '화석', '봤', '더니', '…']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens = twitter.morphs(chosun_eilbo_database['title'][0])\n",
    "print(tokens)\n",
    "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
    "for i in tokens:\n",
    "    print(one_hot_encoding(i, word_to_index=word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "7\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "from padding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  1  8  5]\n",
      " [ 0  0  0  0  1  3  5]\n",
      " [ 0  0  0  0  0  9  2]\n",
      " [ 0  0  0  2  4  3  2]\n",
      " [ 0  0  0  0  0  3  2]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  2]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 0  0  0  1 12  3 13]]\n"
     ]
    }
   ],
   "source": [
    "padded_1 = pad_sequences(encoded)\n",
    "print(padded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_2 = pad_sequences(encoded, padding='post')\n",
    "padded_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n",
      "\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n",
      "\n",
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 7  7  3  2 10]\n",
      " [ 1 12  3 13  0]]\n",
      "\n",
      "14\n",
      "\n",
      "[[ 1  5 14 14 14 14 14]\n",
      " [ 1  8  5 14 14 14 14]\n",
      " [ 1  3  5 14 14 14 14]\n",
      " [ 9  2 14 14 14 14 14]\n",
      " [ 2  4  3  2 14 14 14]\n",
      " [ 3  2 14 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  2 14 14 14 14]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13 14 14 14]]\n"
     ]
    }
   ],
   "source": [
    "padded_3 = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "print(padded_3)\n",
    "print()\n",
    "padded_4 = pad_sequences(encoded, padding='post', truncating='post')\n",
    "print(padded_4)\n",
    "print()\n",
    "padded_5 = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "print(padded_5)\n",
    "print()\n",
    "# 단어 집합의 크기보다 1 큰 숫자를 사용\n",
    "last_value = len(tokenizer.word_index) + 1\n",
    "print(last_value)\n",
    "print()\n",
    "padded_6 = pad_sequences(encoded, padding='post', value=last_value)\n",
    "print(padded_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF를 이용하여 벡터화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag_of_words 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n",
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\n",
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n",
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n",
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n",
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n",
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from bag_of_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'판다.픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다. 판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다. 일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과, 대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다. 화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다.약 만년 전 살았던 고대 판다일루락토스이다. 눈여겨볼은 손목 부위 발견된 돌출뼈다. 연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다가짜 엄지.뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다. 판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고. 그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데, 연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다. 화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다.연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데, 과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다. 왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi = Kiwi()\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(regular_expression_sentence) \n",
    "\n",
    "kospacing_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 0, '픽사': 1, '베이': 2, '통통한': 3, '몸집': 4, '어울리지': 5, '않게': 6, '대나무': 7, '주식': 8, '하는': 9, '독특한': 10, '식성': 11, '최소': 12, '만년': 13, '터': 14, '이어졌을': 15, '가능성': 16, '있다는': 17, '연구': 18, '결과': 19, '나왔다': 20, '일': 21, '현지': 22, '외신': 23, '따르면': 24, '미국': 25, '로스앤젤레스': 26, '자연사': 27, '박물관': 28, '왕샤오밍': 29, '박사': 30, '연': 31, '구': 32, '팀': 33, '은': 34, '최근': 35, '약': 36, '전': 37, '자이언트': 38, '조상': 39, '인': 40, '일루': 41, '락토스': 42, '화': 43, '석': 44, '분석': 45, '한': 46, ',': 47, '잡는데': 48, '쓰이는': 49, '‘': 50, '가짜': 51, '엄지': 52, '’': 53, '존재': 54, '했다고': 55, '밝혔다': 56, '화석': 57, '중국': 58, '남부': 59, '윈난성': 60, '자오퉁시': 61, '슈이': 62, '탕바': 63, '지역': 64, '발굴': 65, '됐다': 66, '살았던': 67, '고대': 68, '이다': 69, '눈': 70, '여겨': 71, '볼': 72, '손목': 73, '부위': 74, '발견': 75, '된': 76, '돌출': 77, '뼈': 78, '다': 79, '가': 80, '진번': 81, '째': 82, '손가락': 83, '일명': 84, '라고': 85, '말': 86, '했다': 87, '뉴시스': 88, '는': 89, '보다': 90, '쉽게': 91, '잡수': 92, '있도록': 93, '데': 94, '쓰인다': 95, '초식': 96, '동물': 97, '진화': 98, '하는데': 99, '중추': 100, '적': 101, '역할': 102, '했을': 103, '전문가': 104, '들': 105, '보고': 106, '그동안': 107, '같은': 108, '적응': 109, '비교': 110, '이뤄진': 111, '여겨져': 112, '왔는데': 113, '무려': 114, '거슬러': 115, '올라간다는': 116, '증명': 117, '해낸': 118, '셈': 119, '현': 120, '대': 121, '더': 122, '긴': 123, '길이': 124, '직선': 125, '모양': 126, '하고': 127, '있었다': 128, '“': 129, '잡고': 130, '뜯어': 131, '먹을는': 132, '먹이': 133, '찾아': 134, '걸어갈': 135, '몸무게': 136, '지탱': 137, '용도': 138, '도': 139, '쓰이는데': 140, '과정': 141, '짧은': 142, '갈고리': 143, '형': 144, '하게': 145, '”': 146, '이라고': 147, '왕': 148, '먹기': 149, '좋게': 150, '쪼개기': 151, '위해': 152, '줄기': 153, '단단히': 154, '붙잡는': 155, '많은': 156, '양대': 157, '나무': 158, '먹는데': 159, '가장': 160, '중요한': 161, '라며': 162, '육식성': 163, '해': 164, '만': 165, '먹는': 166, '종': 167, '바뀐': 168, '장애': 169, '넘어야': 170, '이고': 171, '그': 172, '중': 173, '놀라운': 174, '설명': 175}\n",
      "[11, 1, 1, 1, 1, 1, 1, 6, 1, 3, 1, 1, 1, 5, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 8, 2, 3, 3, 2, 2, 3, 2, 2, 1, 1, 2, 1, 3, 1, 1, 2, 7, 7, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 6, 2, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab, bow = build_bag_of_words(kospacing_sent)\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 9\n",
      "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']\n",
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n",
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from tf_idf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다.픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다. 판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다. 일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과, 대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다. 화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다.약 만년 전 살았던 고대 판다일루락토스이다. 눈여겨볼은 손목 부위 발견된 돌출뼈다. 연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다가짜 엄지.뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다. 판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고. 그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데, 연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다. 화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다.연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데, 과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다. 왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.\n"
     ]
    }
   ],
   "source": [
    "kospacing_sent \n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 769ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = chosun_eilbo_database['title'][0]\n",
    "title = spacing(title)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다', '판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다', '일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과', '대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다', '화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다', '약 만년 전 살았던 고대 판다일루락토스이다', '눈여겨볼은 손목 부위 발견된 돌출뼈다', '연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다', '판다가짜 엄지', '뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다', '판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고', '그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데', '연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다', '화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다', '연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데', '과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다', '왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다', '']\n"
     ]
    }
   ],
   "source": [
    "kospacing_sent_list = []\n",
    "list1 = kospacing_sent.split('.')\n",
    "for i in list1:\n",
    "    i_list1 = i.split(',')\n",
    "    for i_2 in i_list1:\n",
    "        kospacing_sent_list.append(i_2.strip())\n",
    "print(kospacing_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_from_kiwi_analyzed(comment):\n",
    "    temp_sentence = kiwi.analyze(comment)\n",
    "    noun_list = [token.form for token in temp_sentence[0][0] if re.match('N', token.tag)]\n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_noun = []\n",
    "for content in kospacing_sent_list:\n",
    "    for noun in extract_noun_from_kiwi_analyzed(content):\n",
    "        if len(noun) < 2:\n",
    "            continue\n",
    "        else:\n",
    "            content_noun.append(noun)\n",
    "content_noun = list(set(content_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…']\n"
     ]
    }
   ],
   "source": [
    "kospacing_title_list = []\n",
    "list2 = title.split('.')\n",
    "for i in list2:\n",
    "    i_list1 = i.split(',')\n",
    "    for i_2 in i_list1:\n",
    "        kospacing_title_list.append(i_2.strip())\n",
    "print(kospacing_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_noun = []\n",
    "for title in kospacing_title_list:\n",
    "    for noun in extract_noun_from_kiwi_analyzed(title):\n",
    "        if len(noun) < 2:\n",
    "            continue\n",
    "        else:\n",
    "            title_noun.append(noun)\n",
    "title_noun = list(set(content_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 66\n",
      "['가능', '가짜', '갈고리', '결과', '고대', '과정', '그동안', '나무', '남부', '뉴시스', '대나무', '독특', '돌출', '동물', '락토스', '로스앤젤레스', '만년', '먹이', '모양', '몸무게', '몸집', '미국', '박물관', '박사', '발견', '발굴', '베이', '부위', '분석', '설명', '손가락', '손목', '슈이', '식성', '엄지', '역할', '연구', '왕샤오밍', '외신', '용도', '윈난성', '육식성', '자연사', '자오퉁시', '자이언트', '장애', '적응', '전문가', '조상', '존재', '주식', '줄기', '중국', '중추', '증명', '지역', '지탱', '직선', '진화', '초식', '최소', '탕바', '픽사', '현대판', '현지', '화석']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(content_noun+title_noun))\n",
    "vocab.sort()\n",
    "print('단어장의 크기 :', len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "N = len(kospacing_sent_list)\n",
    "title_N = len(kospacing_title_list)\n",
    "print(N)\n",
    "print(title_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_sent_list[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "        \n",
    "content_tf_ = pd.DataFrame(result, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능</th>\n",
       "      <th>가짜</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그동안</th>\n",
       "      <th>나무</th>\n",
       "      <th>남부</th>\n",
       "      <th>뉴시스</th>\n",
       "      <th>...</th>\n",
       "      <th>지탱</th>\n",
       "      <th>직선</th>\n",
       "      <th>진화</th>\n",
       "      <th>초식</th>\n",
       "      <th>최소</th>\n",
       "      <th>탕바</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대판</th>\n",
       "      <th>현지</th>\n",
       "      <th>화석</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    가능  가짜  갈고리  결과  고대  과정  그동안  나무  남부  뉴시스  ...  지탱  직선  진화  초식  최소  탕바  \\\n",
       "0    0   0    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "1    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "2    1   0    0   1   0   0    0   0   0    0  ...   0   0   0   0   1   0   \n",
       "3    0   0    0   1   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "4    0   1    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "5    0   0    0   0   0   0    0   0   1    0  ...   0   0   0   0   0   1   \n",
       "6    0   0    0   0   1   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "7    0   0    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "8    0   1    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "9    0   1    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "10   0   1    0   0   0   0    0   1   0    1  ...   0   0   0   0   0   0   \n",
       "11   0   0    0   0   0   0    0   0   0    0  ...   0   0   1   1   0   0   \n",
       "12   0   0    0   0   0   0    1   0   0    0  ...   0   0   1   0   0   0   \n",
       "13   0   0    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "14   0   1    0   0   0   0    0   0   0    0  ...   0   1   0   0   0   0   \n",
       "15   0   1    0   0   0   0    0   1   0    0  ...   1   0   0   0   0   0   \n",
       "16   0   0    1   0   0   1    0   0   0    0  ...   0   0   1   0   0   0   \n",
       "17   0   1    0   0   0   0    0   3   0    0  ...   0   0   2   0   0   0   \n",
       "18   0   0    0   0   0   0    0   0   0    0  ...   0   0   0   0   0   0   \n",
       "\n",
       "    픽사  현대판  현지  화석  \n",
       "0    0    0   0   0  \n",
       "1    1    0   0   0  \n",
       "2    0    0   0   0  \n",
       "3    0    0   1   0  \n",
       "4    0    0   0   0  \n",
       "5    0    0   0   1  \n",
       "6    0    0   0   0  \n",
       "7    0    0   0   0  \n",
       "8    0    0   0   0  \n",
       "9    0    0   0   0  \n",
       "10   0    0   0   0  \n",
       "11   0    0   0   0  \n",
       "12   0    0   0   0  \n",
       "13   0    0   0   0  \n",
       "14   0    1   0   1  \n",
       "15   0    0   0   0  \n",
       "16   0    0   0   0  \n",
       "17   0    0   0   0  \n",
       "18   0    0   0   0  \n",
       "\n",
       "[19 rows x 66 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>가능</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>가짜</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>갈고리</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>결과</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>고대</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>탕바</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>픽사</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>현대판</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>현지</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>화석</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "가능   1.386294\n",
       "가짜   1.386294\n",
       "갈고리  1.386294\n",
       "결과   1.386294\n",
       "고대   1.386294\n",
       "..        ...\n",
       "탕바   1.386294\n",
       "픽사   1.386294\n",
       "현대판  1.386294\n",
       "현지   1.386294\n",
       "화석   1.386294\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "content_idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "content_idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능</th>\n",
       "      <th>가짜</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그동안</th>\n",
       "      <th>나무</th>\n",
       "      <th>남부</th>\n",
       "      <th>뉴시스</th>\n",
       "      <th>...</th>\n",
       "      <th>지탱</th>\n",
       "      <th>직선</th>\n",
       "      <th>진화</th>\n",
       "      <th>초식</th>\n",
       "      <th>최소</th>\n",
       "      <th>탕바</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대판</th>\n",
       "      <th>현지</th>\n",
       "      <th>화석</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.158883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          가능        가짜       갈고리        결과        고대        과정       그동안  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   1.386294  0.000000  0.000000  1.386294  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  1.386294  0.000000  0.000000  0.000000   \n",
       "4   0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  1.386294  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.386294   \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  1.386294  0.000000  0.000000  1.386294  0.000000   \n",
       "17  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          나무        남부       뉴시스  ...        지탱        직선        진화        초식  \\\n",
       "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1   1.386294  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   1.386294  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  1.386294  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  1.386294  0.000000  1.386294  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  1.386294   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  ...  0.000000  1.386294  0.000000  0.000000   \n",
       "15  1.386294  0.000000  0.000000  ...  1.386294  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "17  4.158883  0.000000  0.000000  ...  0.000000  0.000000  2.772589  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          최소        탕바        픽사       현대판        현지        화석  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  \n",
       "2   1.386294  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  1.386294  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  1.386294  0.000000  0.000000  0.000000  1.386294  \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.000000  0.000000  0.000000  1.386294  0.000000  1.386294  \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[19 rows x 66 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_sent_list[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "content_tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "content_tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_title_list[0]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "        \n",
    "title_tf_ = pd.DataFrame(result, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능</th>\n",
       "      <th>가짜</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그동안</th>\n",
       "      <th>나무</th>\n",
       "      <th>남부</th>\n",
       "      <th>뉴시스</th>\n",
       "      <th>...</th>\n",
       "      <th>지탱</th>\n",
       "      <th>직선</th>\n",
       "      <th>진화</th>\n",
       "      <th>초식</th>\n",
       "      <th>최소</th>\n",
       "      <th>탕바</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대판</th>\n",
       "      <th>현지</th>\n",
       "      <th>화석</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    가능  가짜  갈고리  결과  고대  과정  그동안  나무  남부  뉴시스  ...  지탱  직선  진화  초식  최소  탕바  \\\n",
       "0    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "1    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "2    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "3    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "4    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "5    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "6    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "7    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "8    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "9    0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "10   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "11   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "12   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "13   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "14   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "15   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "16   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "17   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "18   0   0    0   0   0   0    0   1   0    0  ...   0   0   0   0   0   0   \n",
       "\n",
       "    픽사  현대판  현지  화석  \n",
       "0    0    0   0   1  \n",
       "1    0    0   0   1  \n",
       "2    0    0   0   1  \n",
       "3    0    0   0   1  \n",
       "4    0    0   0   1  \n",
       "5    0    0   0   1  \n",
       "6    0    0   0   1  \n",
       "7    0    0   0   1  \n",
       "8    0    0   0   1  \n",
       "9    0    0   0   1  \n",
       "10   0    0   0   1  \n",
       "11   0    0   0   1  \n",
       "12   0    0   0   1  \n",
       "13   0    0   0   1  \n",
       "14   0    0   0   1  \n",
       "15   0    0   0   1  \n",
       "16   0    0   0   1  \n",
       "17   0    0   0   1  \n",
       "18   0    0   0   1  \n",
       "\n",
       "[19 rows x 66 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tf_.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>가능</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>가짜</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>갈고리</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>결과</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>고대</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>탕바</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>픽사</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>현대판</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>현지</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>화석</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "가능   1.386294\n",
       "가짜   1.386294\n",
       "갈고리  1.386294\n",
       "결과   1.386294\n",
       "고대   1.386294\n",
       "..        ...\n",
       "탕바   1.386294\n",
       "픽사   1.386294\n",
       "현대판  1.386294\n",
       "현지   1.386294\n",
       "화석   1.386294\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "title_idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "title_idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_title_list[0]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t, d))\n",
    "        \n",
    "title_tfidf_ = pd.DataFrame(result, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능</th>\n",
       "      <th>가짜</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그동안</th>\n",
       "      <th>나무</th>\n",
       "      <th>남부</th>\n",
       "      <th>뉴시스</th>\n",
       "      <th>...</th>\n",
       "      <th>지탱</th>\n",
       "      <th>직선</th>\n",
       "      <th>진화</th>\n",
       "      <th>초식</th>\n",
       "      <th>최소</th>\n",
       "      <th>탕바</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대판</th>\n",
       "      <th>현지</th>\n",
       "      <th>화석</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     가능   가짜  갈고리   결과   고대   과정  그동안        나무   남부  뉴시스  ...   지탱   직선   진화  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "     초식   최소   탕바   픽사  현대판   현지        화석  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  1.386294  \n",
       "\n",
       "[19 rows x 66 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1과 문서2의 유사도 : 0.6666666666666667\n",
      "문서 1과 문서3의 유사도 : 0.6666666666666667\n",
      "문서 2와 문서3의 유사도 : 1.0000000000000002\n",
      "overview 열의 결측값의 수: 135\n",
      "TF-IDF 행렬의 크기(shape) : (20000, 47487)\n",
      "코사인 유사도 연산 결과 : (20000, 20000)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from cosine_similarity import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(title_tf_.values)\n",
    "# np.linalg.det(title_tf_.values, content_tf_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (19,66) and (19,66) not aligned: 66 (dim 1) != 19 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\bigdata_git\\bigdata_thunder\\bigdata2\\practice_bigdata_project.ipynb 셀 85\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cos_sim(title_tf_\u001b[39m.\u001b[39;49mvalues, content_tf_\u001b[39m.\u001b[39;49mvalues)\n",
      "File \u001b[1;32mc:\\bigdata_git\\bigdata_thunder\\bigdata2\\cosine_similarity.py:21\u001b[0m, in \u001b[0;36mcos_sim\u001b[1;34m(A, B)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcos_sim\u001b[39m(A, B):\n\u001b[1;32m---> 21\u001b[0m   \u001b[39mreturn\u001b[39;00m dot(A, B)\u001b[39m/\u001b[39m(norm(A)\u001b[39m*\u001b[39mnorm(B))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (19,66) and (19,66) not aligned: 66 (dim 1) != 19 (dim 0)"
     ]
    }
   ],
   "source": [
    "cos_sim(title_tf_.values, content_tf_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0],\n",
       "       [0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 6, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(title_tf_.values, content_tf_.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.717797887081348"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(title_tf_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.224972160321824"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(content_tf_.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = dot(title_tf_.values, content_tf_.values.T)/(norm(title_tf_.values)*norm(content_tf_.values.T))\n",
    "cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(title_tf_.values, content_tf_.values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chosun_eilbo_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\bigdata_git\\bigdata_thunder\\bigdata2\\practice_bigdata_project.ipynb 셀 95\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#Y152sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m newspaper_title \u001b[39min\u001b[39;00m chosun_eilbo_database\u001b[39m.\u001b[39mtitle:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#Y152sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     newspaper_title_0 \u001b[39m=\u001b[39m newspaper_title\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/bigdata_git/bigdata_thunder/bigdata2/practice_bigdata_project.ipynb#Y152sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     kospacing_new_title_0_data \u001b[39m=\u001b[39m spacing(newspaper_title_0) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'chosun_eilbo_database' is not defined"
     ]
    }
   ],
   "source": [
    "for newspaper_title in chosun_eilbo_database[\"title\"]:\n",
    "    newspaper_title_0 = newspaper_title.replace(\" \", '')\n",
    "    kospacing_new_title_0_data = spacing(newspaper_title_0) \n",
    "    print(kospacing_new_title_0_data)\n",
    "    with open(\"C:\\\\bigdata_git\\\\bigdata_thunder\\\\bigdata2\\\\korean_stopwords.txt\", \"r\", encoding=\"UTF-8\") as stopwords:\n",
    "        stopwords_list = stopwords.read().split('\\n')\n",
    "    new_title_1 ='' \n",
    "    for newspaper_title_word in korean_change_no_stop_words(kospacing_new_title_0_data, stopwords_list):\n",
    "        new_title_1 += newspaper_title_word\n",
    "    tokenizer3_title_1 = RegexpTokenizer('[A-Za-z0-9ㄱ-ㅎ/()]+', gaps=True)\n",
    "    title_regular_expression_sentence = ''\n",
    "    for expression in tokenizer3_content_1.tokenize(new_content_1):\n",
    "        title_regular_expression_sentence += expression\n",
    "    title_vocab = {}\n",
    "    title_preprocessed_sentences = []\n",
    "    result = []\n",
    "    for title_sentence in sent_tokenize(title_regular_expression_sentence):\n",
    "        # 단어 토큰화\n",
    "        title_tokenized_sentence = twitter.nouns(title_sentence)\n",
    "\n",
    "        for word in title_tokenized_sentence: \n",
    "            if len(word) > 2: \n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "        preprocessed_sentences.append(result) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".replace(\" \", '')\n",
    "\n",
    "kospacing_new_content_0_data = spacing(new_content_0_data) \n",
    "print(kospacing_new_content_0_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
