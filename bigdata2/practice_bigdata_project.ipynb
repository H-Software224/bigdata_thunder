{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER 뉴스 데이터를 이용하여서 데이터 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup, Requests, Pandas 모듈 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터베이스 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스별 데이터베이스 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosun_eilbo_database = pd.read_csv(r'C:\\Users\\user\\Documents\\github\\bigdata_thunder\\bigdata1\\chosun_eilbo.csv')\n",
    "korean_economy_database = pd.read_csv(r'C:\\Users\\user\\Documents\\github\\bigdata_thunder\\bigdata1\\korean_economy.csv')\n",
    "mail_economy_database = pd.read_csv(r'C:\\Users\\user\\Documents\\github\\bigdata_thunder\\bigdata1\\mail_economy.csv')\n",
    "midlle_eilbo_database = pd.read_csv(r'C:\\Users\\user\\Documents\\github\\bigdata_thunder\\bigdata1\\middle_eilbo.csv')\n",
    "money_today_database = pd.read_csv(r'C:\\Users\\user\\Documents\\github\\bigdata_thunder\\bigdata1\\money_today.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "데이터베이스 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…</td>\n",
       "      <td>통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이준석 변호인 징계요청 기각… 경기변호사회 “수사결과 봐야”</td>\n",
       "      <td>“가세연의 이준석 녹취는 일부 삭제된 것” 경기중앙지방변호사회가 한 보수단체가 제기...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박지현 “이광재가 내 배후? 어리면 배후 있을 거라는 꼰대식 사고”</td>\n",
       "      <td>비대위원장직 자진사퇴 후 한 달 만에 공개행사 참석 이재명 전당대회 출마와 민형배 ...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>野 당권주자들 “검수완박 성급했다” 이제와서 반성 모드</td>\n",
       "      <td>강병원 “진영 논리서 벗어나야” 박용진 “상식 복원하는 게 혁신” 꼼수 탈당한 민형...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“먹으니 시야 흐려져” 시누이 부부 음식에 메탄올 넣은 30대여성</td>\n",
       "      <td>시누이 부부가 먹을 음식에 유독성 물질을 넣은 30대 여성이 특수상해 혐의로 경찰에...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13688</th>\n",
       "      <td>람보르기니 몰고 온 죄… 러 슈퍼카 차주들, 수갑 찬 채 끌려갔다</td>\n",
       "      <td>러시아에서 값비싼 슈퍼카를 몰던 차주들이 무더기로 경찰에 체포되는 일이 발생했다. ...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>10세 세계 최연소 트랜스젠더 모델…패션계가 주목</td>\n",
       "      <td>세계에서 가장 어린 트랜스젠더 모델인 미국 소녀 노엘라 맥마허(10)가 패션계의 주...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13690</th>\n",
       "      <td>[수요동물원] 살아있는 벌새의 뇌를… 소름 끼치는 사마귀의 먹방</td>\n",
       "      <td>곤충만 먹는줄 알았더니 개구리·도마뱀·새까지 먹어치워 짝짓기 중 동족포식하는 ‘이 ...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13691</th>\n",
       "      <td>[김한수의 오마이갓] 막 오른 ‘서울-로마 두 추기경 시대’</td>\n",
       "      <td>한국 천주교계에 ‘서울·로마 두 추기경 시대’의 막이 올랐습니다. 지난 27일(현지...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13692</th>\n",
       "      <td>[차현진의 돈과 세상] [86] 탕평책</td>\n",
       "      <td>베토벤이 청력을 잃어 불우했다고 하지만, 사실 인복은 많았다. 그는 커피를 끓이기 ...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/023/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13693 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0        판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…   \n",
       "1          이준석 변호인 징계요청 기각… 경기변호사회 “수사결과 봐야”   \n",
       "2      박지현 “이광재가 내 배후? 어리면 배후 있을 거라는 꼰대식 사고”   \n",
       "3             野 당권주자들 “검수완박 성급했다” 이제와서 반성 모드   \n",
       "4       “먹으니 시야 흐려져” 시누이 부부 음식에 메탄올 넣은 30대여성   \n",
       "...                                      ...   \n",
       "13688   람보르기니 몰고 온 죄… 러 슈퍼카 차주들, 수갑 찬 채 끌려갔다   \n",
       "13689            10세 세계 최연소 트랜스젠더 모델…패션계가 주목   \n",
       "13690    [수요동물원] 살아있는 벌새의 뇌를… 소름 끼치는 사마귀의 먹방   \n",
       "13691      [김한수의 오마이갓] 막 오른 ‘서울-로마 두 추기경 시대’   \n",
       "13692                  [차현진의 돈과 세상] [86] 탕평책   \n",
       "\n",
       "                                                 content  \\\n",
       "0      통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성...   \n",
       "1      “가세연의 이준석 녹취는 일부 삭제된 것” 경기중앙지방변호사회가 한 보수단체가 제기...   \n",
       "2      비대위원장직 자진사퇴 후 한 달 만에 공개행사 참석 이재명 전당대회 출마와 민형배 ...   \n",
       "3      강병원 “진영 논리서 벗어나야” 박용진 “상식 복원하는 게 혁신” 꼼수 탈당한 민형...   \n",
       "4      시누이 부부가 먹을 음식에 유독성 물질을 넣은 30대 여성이 특수상해 혐의로 경찰에...   \n",
       "...                                                  ...   \n",
       "13688  러시아에서 값비싼 슈퍼카를 몰던 차주들이 무더기로 경찰에 체포되는 일이 발생했다. ...   \n",
       "13689  세계에서 가장 어린 트랜스젠더 모델인 미국 소녀 노엘라 맥마허(10)가 패션계의 주...   \n",
       "13690  곤충만 먹는줄 알았더니 개구리·도마뱀·새까지 먹어치워 짝짓기 중 동족포식하는 ‘이 ...   \n",
       "13691  한국 천주교계에 ‘서울·로마 두 추기경 시대’의 막이 올랐습니다. 지난 27일(현지...   \n",
       "13692  베토벤이 청력을 잃어 불우했다고 하지만, 사실 인복은 많았다. 그는 커피를 끓이기 ...   \n",
       "\n",
       "                                                     url  \n",
       "0      https://n.news.naver.com/mnews/article/023/000...  \n",
       "1      https://n.news.naver.com/mnews/article/023/000...  \n",
       "2      https://n.news.naver.com/mnews/article/023/000...  \n",
       "3      https://n.news.naver.com/mnews/article/023/000...  \n",
       "4      https://n.news.naver.com/mnews/article/023/000...  \n",
       "...                                                  ...  \n",
       "13688  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13689  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13690  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13691  https://n.news.naver.com/mnews/article/023/000...  \n",
       "13692  https://n.news.naver.com/mnews/article/023/000...  \n",
       "\n",
       "[13693 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표본별 뉴스 제목중 하나 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'판다는 언제부터 대나무만 먹었을까? 600만년 전 화석 봤더니…'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 출처 홈페이지 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://n.news.naver.com/mnews/article/023/0003701167'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosun_eilbo_database['url'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깃허브 오픈소스 이용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화로 바꾸는 파일 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ git clone https://github.com/ukairia777/tensorflow-nlp-tutorial.git\n",
    "# $ git clone https://github.com/haven-jeon/PyKoSpacing.git\n",
    "# $ git clone https://github.com/ssut/py-hanspell.git\n",
    "# %pip install soynlp\n",
    "# %pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\users\\user\\anaconda3\\lib\\site-packages (3.6.4)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\anaconda3\\lib\\site-packages (from kss) (2022.3.15)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from kss) (1.2.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\user\\anaconda3\\lib\\site-packages (from kss) (8.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soynlp in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0.493)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (5.8.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: customized_konlpy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0.64)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from customized_konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n",
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n",
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n",
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n",
      "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "19  1990  52 1 22\n",
      "오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
      "테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n",
      "training was done. used memory 4.303 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n",
      "0.08838002913645132\n",
      "0.19841268168224552\n",
      "0.2972877884078849\n",
      "0.37891487632839754\n",
      "0.33492963377557666\n",
      "1.6371694761537934\n",
      "-0.0\n",
      "-0.0\n",
      "3.1400392861792916\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing_tools_for_korean_text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url 홈페이지 BeautifulSoup을이용해서 소스에 있는 내용 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다. /픽사베이통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성이 최소 600만년 전부터 이어졌을 가능성이 있다는 연구 결과가 나왔다.1일(현지 시각) CNN 등 외신에 따르면 미국 로스앤젤레스 자연사 박물관의 왕 샤오밍 박사 연구팀은 최근 약 600만년 전 자이언트 판다 조상인 아일루락토스(Ailurarctos) 화석을 분석한 결과, 대나무를 잡는 데 쓰이는 ‘가짜 엄지’가 존재했다고 밝혔다.이 화석은 중국 남부 윈난성 자오퉁시의 슈이탕바 지역에서 발굴됐다. 약 600만년 전 살았던 고대 판다 아일루락토스의 것이다. 눈여겨볼 것은 손목 부위에서 발견된 돌출 뼈다. 연구팀은 바로 이 뼈가 자이언트 판다가 가진 여섯 번째 손가락 일명 ‘가짜 엄지’라고 말했다.판다의 가짜 엄지. /뉴시스판다의 가짜 엄지는 대나무를 보다 쉽게 잡을 수 있도록 하는 데 쓰인다. 판다가 초식동물로 진화하는 데 중추적인 역할을 했을 것으로 전문가들은 보고 있다. 그동안 이같은 진화적 적응은 약 15만년 전인 비교적 최근 이뤄진 것으로 여겨져 왔는데, 이번 연구가 무려 600만년을 거슬러 올라간다는 것을 증명해낸 셈이다.화석에서 발견된 가짜 엄지는 현대 판다의 것보다 더 긴 길이의 직선 모양을 하고 있었다. 연구팀은 “가짜 엄지는 대나무를 잡고 뜯어먹을 때는 물론 다음 먹이를 찾아 걸어갈 때 몸무게를 지탱하는 용도로도 쓰이는데, 이 과정에서 긴 뼈가 짧은 갈고리형으로 진화하게 된 것”이라고 분석했다.왕 박사는 “대나무를 먹기 좋게 쪼개기 위해 줄기를 단단히 붙잡는 것은 많은 양의 대나무를 먹는 데 가장 중요한 적응”이라며 “육식성 조상에서 진화해 대나무만 먹는 종으로 바뀐 판다는 많은 장애를 넘어야 했을 것이고 가짜 엄지는 그중 가장 놀라운 진화”라고 설명했다.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "판다. / 픽사베이통통한 몸집과 어울리지 않게 대나무를 주식으로 하는 판다. 이런 판다의 독특한 식성이 최소 600만년 전부터 이어졌을 가능성이 있다는 연구 결과가 나왔다.1일(현지시각) CNN 등 외신에 따르면 미국 로스앤젤레스 자연사박물관의 왕샤오밍 박사 연구팀은 최근 약 600만년 전자이언트 판다 조상인 아일루락토스(A ilurarctos) 화석을 분석한 결과, 대나무를 잡는 데 쓰이는 ‘가짜 엄지’가 존재했다고 밝혔다.이 화석은 중국 남부 윈 난성자오퉁시의 슈이 탕바지역에서 발굴됐다. 약 600만년 전 살았던 고대 판다 아일루락토스의 것이다. 눈여겨볼 것은 손목 부위에서 발견된 돌출뼈다. 연구팀은 바로 이 뼈가 자이언트 판다가 가진 여섯 번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다의 가짜 엄지. / 뉴시스판다의 가짜 엄지는 대나무를 보다 쉽게 잡을 수 있도록 하는데 쓰인다. 판다가 초식동물로 진화하는 데 중추적인 역할을 했을 것으로 전문가들은 보고 있다. 그동안 이 같은 진화적 적응은 약 15만년 전 인 비교적 최근 이뤄진 것으로 여겨져 왔는데, 이번 연구가 무려 600만년을 거슬러 올라간다는 것을 증명해낸 셈이다. 화석에서 발견된 가짜 엄지는 현대판다의 것보다 더 긴 길이의 직선 모양을 하고 있었다. 연구팀은 “가짜 엄지는 대나무를 잡고 뜯어먹을 때는 물론 다음 먹이를 찾아 걸어갈 때 몸무게를 지탱하는 용도로도 쓰이는데, 이 과정에서 긴 뼈가 짧은 갈고리형으로 진화하게 된 것”이라고 분석했다. 왕 박사는 “대나무를 먹기 좋게 쪼개기 위해 줄기를 단단히 붙잡는 것은 많은 양의 대나무를 먹는 데 가장 중요한 적응”이라며 “육식성 조상에서 진화해 대나무만 먹는 종으로 바뀐 판다는 많은 장애를 넘어야 했을 것이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.\n"
     ]
    }
   ],
   "source": [
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"}\n",
    "response_0 = requests.get(chosun_eilbo_database[\"url\"][0], headers=headers)\n",
    "soup_0 = BeautifulSoup(response_0.text, 'html.parser')\n",
    "content_0_data = soup_0.find('div', {\"id\" : \"dic_area\"}).get_text()\n",
    "content_0_data = content_0_data.replace('\\n','').replace('\\t','')\n",
    "print(content_0_data)\n",
    "new_content_0_data = content_0_data.replace(\" \", '')\n",
    "\n",
    "kospacing_new_content_0_data = spacing(new_content_0_data) \n",
    "print(kospacing_new_content_0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '과', '어울', '리지', '않게', '대나무', '를', '주식', '으로', '하는', '판다', '.', '이런', '판다', '의', '독특', '한', '식성', '이', '최소', '600', '만년', '전부', '터', '이어졌을', '가능성', '이', '있다는', '연구', '결과', '가', '나왔다', '.', '1일', '(', '현지', '시각', ')', 'CNN', '등', '외신', '에', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '의', '왕샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '아', '일루', '락토스', '(', 'A', 'ilurarctos', ')', '화석', '을', '분석', '한', '결과', ',', '대나무', '를', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '가', '존재', '했다고', '밝혔다', '.', '이', '화석', '은', '중국', '남부', '윈', '난', '성', '자오퉁시', '의', '슈이', '탕바', '지역', '에서', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '아', '일루', '락토스', '의', '것', '이다', '.', '눈', '여겨', '볼', '것', '은', '손목', '부위', '에서', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '바로', '이', '뼈', '가', '자이언트', '판다', '가', '가진', '여섯', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '의', '가짜', '엄지', '.', '/', '뉴시스', '판다', '의', '가짜', '엄지', '는', '대나무', '를', '보다', '쉽게', '잡', '을', '수', '있', '도록', '하는데', '쓰인다', '.', '판다', '가', '초식동물', '로', '진화', '하는', '데', '중', '추적', '인', '역할', '을', '했을', '것', '으로', '전문가', '들은', '보고', '있다', '.', '그동안', '이', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전', '인', '비교', '적', '최근', '이뤄진', '것', '으로', '여겨져', '왔는데', ',', '이번', '연구', '가', '무려', '600', '만년', '을', '거슬러', '올라', '간다', '는', '것', '을', '증명', '해', '낸', '셈', '이다', '.', '화석', '에서', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '의', '것', '보다', '더', '긴', '길이', '의', '직선', '모양', '을', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '를', '잡고', '뜯어', '먹을', '때', '는', '물론', '다음', '먹이', '를', '찾아', '걸어갈', '때', '몸무게', '를', '지탱', '하는', '용도', '로', '도', '쓰이는데', ',', '이', '과정', '에서', '긴', '뼈', '가', '짧은', '갈고리', '형', '으로', '진화', '하게', '된', '것', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '를', '먹기', '좋게', '쪼개기', '위해', '줄기', '를', '단단히', '붙잡는', '것', '은', '많은', '양', '의', '대나무', '를', '먹는', '데', '가장', '중요', '한', '적응', '”', '이', '라며', '“', '육식성', '조상', '에서', '진화', '해', '대나무', '만', '먹는', '종', '으로', '바뀐', '판다', '는', '많은', '장애', '를', '넘어야', '했을', '것', '이고', '가짜', '엄지', '는', '그', '중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs(kospacing_new_content_0_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = twitter.morphs(content_0_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 제거하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
      "die\n",
      "watch\n",
      "have\n",
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n",
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from stemming_lemmatization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for word in words_data:\n",
    "#     print(lemmatizer.lemmatize(word))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 내용항목 중 한국어 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from remove_stopwords import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '과', '어울', '리지', '않게', '대나무', '를', '주식', '으로', '하는', '판다', '.', '이런', '판다', '의', '독특', '한', '식성', '이', '최소', '600', '만년', '전부', '터', '이어졌을', '가능성', '이', '있다는', '연구', '결과', '가', '나왔다', '.', '1일', '(', '현지', '시각', ')', 'CNN', '등', '외신', '에', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '의', '왕', '샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '아', '일루', '락토스', '(', 'Ailurarctos', ')', '화석', '을', '분석', '한', '결과', ',', '대나무', '를', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '가', '존재', '했다고', '밝혔다', '.', '이', '화석', '은', '중국', '남부', '윈난성', '자오퉁시', '의', '슈이탕바', '지역', '에서', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '아', '일루', '락토스', '의', '것', '이다', '.', '눈', '여겨', '볼', '것', '은', '손목', '부위', '에서', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '바로', '이', '뼈', '가', '자이언트', '판다', '가', '가진', '여섯', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '의', '가짜', '엄지', '.', '/', '뉴시스', '판다', '의', '가짜', '엄지', '는', '대나무', '를', '보다', '쉽게', '잡', '을', '수', '있', '도록', '하는', '데', '쓰인다', '.', '판다', '가', '초식동물', '로', '진화', '하는', '데', '중', '추적', '인', '역할', '을', '했을', '것', '으로', '전문가', '들은', '보고', '있다', '.', '그동안', '이', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전인', '비교', '적', '최근', '이뤄진', '것', '으로', '여겨져', '왔는데', ',', '이번', '연구', '가', '무려', '600', '만년', '을', '거슬러', '올라', '간다', '는', '것', '을', '증명', '해', '낸', '셈', '이다', '.', '화석', '에서', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '의', '것', '보다', '더', '긴', '길이', '의', '직선', '모양', '을', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '를', '잡고', '뜯어', '먹을', '때', '는', '물론', '다음', '먹이', '를', '찾아', '걸어갈', '때', '몸무게', '를', '지탱', '하는', '용도', '로', '도', '쓰이는데', ',', '이', '과정', '에서', '긴', '뼈', '가', '짧은', '갈고리', '형', '으로', '진화', '하게', '된', '것', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '를', '먹기', '좋게', '쪼개기', '위해', '줄기', '를', '단단히', '붙잡는', '것', '은', '많은', '양', '의', '대나무', '를', '먹는', '데', '가장', '중요', '한', '적응', '”', '이', '라며', '“', '육식성', '조상', '에서', '진화', '해', '대나무', '만', '먹는', '종', '으로', '바뀐', '판다', '는', '많은', '장애', '를', '넘어야', '했을', '것', '이고', '가짜', '엄지', '는', '그중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n",
      "\n",
      "불용어 제거 후 : ['판다', '.', '/', '픽사', '베이', '통통', '한', '몸집', '어울', '리지', '않게', '대나무', '주식', '하는', '판다', '.', '판다', '독특', '한', '식성', '최소', '600', '만년', '터', '이어졌을', '가능성', '있다는', '연구', '결과', '나왔다', '.', '1일', '(', '현지', ')', 'CNN', '외신', '따르면', '미국', '로스앤젤레스', '자연사', '박물관', '왕', '샤오밍', '박사', '연구', '팀', '은', '최근', '약', '600', '만년', '전', '자이언트', '판다', '조상', '인', '일루', '락토스', '(', 'Ailurarctos', ')', '화석', '분석', '한', '결과', ',', '대나무', '잡는', '데', '쓰이는', '‘', '가짜', '엄지', '’', '존재', '했다고', '밝혔다', '.', '화석', '은', '중국', '남부', '윈난성', '자오퉁시', '슈이탕바', '지역', '발굴', '됐다', '.', '약', '600', '만년', '전', '살았던', '고대', '판다', '일루', '락토스', '이다', '.', '눈', '여겨', '볼', '은', '손목', '부위', '발견', '된', '돌출', '뼈', '다', '.', '연구', '팀', '은', '뼈', '자이언트', '판다', '가진', '번째', '손가락', '일명', '‘', '가짜', '엄지', '’', '라고', '말', '했다', '.', '판다', '가짜', '엄지', '.', '/', '뉴시스', '판다', '가짜', '엄지', '는', '대나무', '보다', '쉽게', '잡', '수', '있', '도록', '하는', '데', '쓰인다', '.', '판다', '초식동물', '진화', '하는', '데', '중', '추적', '인', '역할', '했을', '전문가', '들은', '보고', '.', '그동안', '같은', '진화', '적', '적응', '은', '약', '15', '만년', '전인', '비교', '적', '최근', '이뤄진', '여겨져', '왔는데', ',', '연구', '무려', '600', '만년', '거슬러', '올라', '간다', '는', '증명', '해', '낸', '셈', '이다', '.', '화석', '발견', '된', '가짜', '엄지', '는', '현대', '판다', '보다', '더', '긴', '길이', '직선', '모양', '하고', '있었다', '.', '연구', '팀', '은', '“', '가짜', '엄지', '는', '대나무', '잡고', '뜯어', '먹을', '는', '먹이', '찾아', '걸어갈', '몸무게', '지탱', '하는', '용도', '도', '쓰이는데', ',', '과정', '긴', '뼈', '짧은', '갈고리', '형', '진화', '하게', '된', '”', '이라고', '분석', '했다', '.', '왕', '박사', '는', '“', '대나무', '먹기', '좋게', '쪼개기', '위해', '줄기', '단단히', '붙잡는', '은', '많은', '양', '대나무', '먹는', '데', '가장', '중요', '한', '적응', '”', '라며', '“', '육식성', '조상', '진화', '해', '대나무', '만', '먹는', '종', '바뀐', '판다', '는', '많은', '장애', '넘어야', '했을', '이고', '가짜', '엄지', '는', '그중', '가장', '놀라운', '진화', '”', '라고', '설명', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:\\\\Users\\\\user\\\\Documents\\\\github\\\\bigdata_thunder\\\\bigdata2\\\\korean_stopwords.txt\", \"r\", encoding=\"UTF-8\") as stopwords:\n",
    "    stopwords_list = stopwords.read().split('\\n')\n",
    "print('불용어 제거 전 :',words_data) \n",
    "print()\n",
    "print('불용어 제거 후 :',korean_change_no_stop_words(words_data, stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다./픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소600만년터이어졌을가능성있다는연구결과나왔다.1일(현지)CNN외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약600만년전자이언트판다조상인일루락토스(Ailurarctos)화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약600만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지./뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약15만년전인비교적최근이뤄진여겨져왔는데,연구무려600만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.\n"
     ]
    }
   ],
   "source": [
    "new_content_1 ='' \n",
    "for i in korean_change_no_stop_words(words_data, stopwords_list):\n",
    "    new_content_1 += i\n",
    "print(new_content_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 텍스트 정규화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n",
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from regular_expression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다.', '픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소', '만년터이어졌을가능성있다는연구결과나왔다.', '일', '현지', '외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약', '만년전자이언트판다조상인일루락토스', '화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약', '만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지.', '뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약', '만년전인비교적최근이뤄진여겨져왔는데,연구무려', '만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer3_content_1 = RegexpTokenizer('[A-Za-z0-9ㄱ-ㅎ/()]+', gaps=True)\n",
    "# tokenizer3_content_2 = RegexpTokenizer('')\n",
    "print(tokenizer3_content_1.tokenize(new_content_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다.픽사베이통통한몸집어울리지않게대나무주식하는판다.판다독특한식성최소만년터이어졌을가능성있다는연구결과나왔다.일현지외신따르면미국로스앤젤레스자연사박물관왕샤오밍박사연구팀은최근약만년전자이언트판다조상인일루락토스화석분석한결과,대나무잡는데쓰이는‘가짜엄지’존재했다고밝혔다.화석은중국남부윈난성자오퉁시슈이탕바지역발굴됐다.약만년전살았던고대판다일루락토스이다.눈여겨볼은손목부위발견된돌출뼈다.연구팀은뼈자이언트판다가진번째손가락일명‘가짜엄지’라고말했다.판다가짜엄지.뉴시스판다가짜엄지는대나무보다쉽게잡수있도록하는데쓰인다.판다초식동물진화하는데중추적인역할했을전문가들은보고.그동안같은진화적적응은약만년전인비교적최근이뤄진여겨져왔는데,연구무려만년거슬러올라간다는증명해낸셈이다.화석발견된가짜엄지는현대판다보다더긴길이직선모양하고있었다.연구팀은“가짜엄지는대나무잡고뜯어먹을는먹이찾아걸어갈몸무게지탱하는용도도쓰이는데,과정긴뼈짧은갈고리형진화하게된”이라고분석했다.왕박사는“대나무먹기좋게쪼개기위해줄기단단히붙잡는은많은양대나무먹는데가장중요한적응”라며“육식성조상진화해대나무만먹는종바뀐판다는많은장애넘어야했을이고가짜엄지는그중가장놀라운진화”라고설명했다.\n"
     ]
    }
   ],
   "source": [
    "regular_expression_sentence = ''\n",
    "for expression in tokenizer3_content_1.tokenize(new_content_1):\n",
    "    regular_expression_sentence += expression\n",
    "print(regular_expression_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n",
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n",
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n",
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n",
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "8\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n",
      "단어 OOV의 인덱스 : 1\n",
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from integer_encoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'went', 'huge', 'mountain', '판다', '픽사', '베이', '통통', '몸집', '어울', '리지', '대나무', '주식', '판다', '판다', '독특', '식성', '최소', '년터', '가능성', '연구', '결과', '현지', '외신', '미국', '로스앤젤레스', '자연사', '박물관', '왕샤오밍', '박사', '연구', '최근', '만년', '자이언트판다', '조상', '일루', '락토스', '화석', '분석', '결과', '대나무', '데쓰', '가짜', '엄지', '존재', '화석', '중국', '남부', '윈난성', '자오퉁시', '슈이탕바', '지역', '발굴', '만년', '고대', '판다', '일루', '락토스', '손목', '부위', '발견', '돌출', '연구', '자이언트판다', '진번', '손가락', '일명', '가짜', '엄지', '판다', '가짜', '엄지', '뉴시스', '판다', '엄지', '대나무', '도록', '데쓰', '판다', '초식동물', '진화', '추적', '역할', '전문가', '보고', '그동안', '진화', '적적', '만년', '전인', '비교', '최근', '진여', '연구', '무려', '년거슬러올', '간다', '증명', '화석', '발견', '가짜', '엄지', '현대', '판다', '길이', '직선', '모양', '연구', '가짜', '엄지', '대나무', '잡고', '먹이', '몸무게', '지탱', '용도', '과정', '갈고리', '진화', '고분', '박사', '대나무', '먹기', '위해', '줄기', '대나무', '가장', '중요', '적응', '라며', '육식성', '조상', '화해', '대나무', '판다', '은장', '가짜', '엄지', '가장', '진화', '설명']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for sentence in sent_tokenize(regular_expression_sentence):\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = twitter.nouns(sentence)\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        if len(word) >=2: \n",
    "            result.append(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0 \n",
    "            vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 9, '픽사': 1, '베이': 1, '통통': 1, '몸집': 1, '어울': 1, '리지': 1, '대나무': 7, '주식': 1, '독특': 1, '식성': 1, '최소': 1, '년터': 1, '가능성': 1, '연구': 5, '결과': 2, '현지': 1, '외신': 1, '미국': 1, '로스앤젤레스': 1, '자연사': 1, '박물관': 1, '왕샤오밍': 1, '박사': 2, '최근': 2, '만년': 3, '자이언트판다': 2, '조상': 2, '일루': 2, '락토스': 2, '화석': 3, '분석': 1, '데쓰': 2, '가짜': 6, '엄지': 7, '존재': 1, '중국': 1, '남부': 1, '윈난성': 1, '자오퉁시': 1, '슈이탕바': 1, '지역': 1, '발굴': 1, '고대': 1, '손목': 1, '부위': 1, '발견': 2, '돌출': 1, '진번': 1, '손가락': 1, '일명': 1, '뉴시스': 1, '도록': 1, '초식동물': 1, '진화': 4, '추적': 1, '역할': 1, '전문가': 1, '보고': 1, '그동안': 1, '적적': 1, '전인': 1, '비교': 1, '진여': 1, '무려': 1, '년거슬러올': 1, '간다': 1, '증명': 1, '현대': 1, '길이': 1, '직선': 1, '모양': 1, '잡고': 1, '먹이': 1, '몸무게': 1, '지탱': 1, '용도': 1, '과정': 1, '갈고리': 1, '고분': 1, '먹기': 1, '위해': 1, '줄기': 1, '가장': 2, '중요': 1, '적응': 1, '라며': 1, '육식성': 1, '화해': 1, '은장': 1, '설명': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'went', 'huge', 'mountain', '판다', '픽사', '베이', '통통', '몸집', '어울', '리지', '대나무', '주식', '판다', '판다', '독특', '식성', '최소', '년터', '가능성', '연구', '결과', '현지', '외신', '미국', '로스앤젤레스', '자연사', '박물관', '왕샤오밍', '박사', '연구', '최근', '만년', '자이언트판다', '조상', '일루', '락토스', '화석', '분석', '결과', '대나무', '데쓰', '가짜', '엄지', '존재', '화석', '중국', '남부', '윈난성', '자오퉁시', '슈이탕바', '지역', '발굴', '만년', '고대', '판다', '일루', '락토스', '손목', '부위', '발견', '돌출', '연구', '자이언트판다', '진번', '손가락', '일명', '가짜', '엄지', '판다', '가짜', '엄지', '뉴시스', '판다', '엄지', '대나무', '도록', '데쓰', '판다', '초식동물', '진화', '추적', '역할', '전문가', '보고', '그동안', '진화', '적적', '만년', '전인', '비교', '최근', '진여', '연구', '무려', '년거슬러올', '간다', '증명', '화석', '발견', '가짜', '엄지', '현대', '판다', '길이', '직선', '모양', '연구', '가짜', '엄지', '대나무', '잡고', '먹이', '몸무게', '지탱', '용도', '과정', '갈고리', '진화', '고분', '박사', '대나무', '먹기', '위해', '줄기', '대나무', '가장', '중요', '적응', '라며', '육식성', '조상', '화해', '대나무', '판다', '은장', '가짜', '엄지', '가장', '진화', '설명']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'판다': 9, '대나무': 7, '엄지': 7, '가짜': 6, '연구': 5, '진화': 4, '만년': 3, '화석': 3, '결과': 2, '박사': 2, '최근': 2, '자이언트판다': 2, '조상': 2, '일루': 2, '락토스': 2, '데쓰': 2, '발견': 2, '가장': 2, 'barber': 1, 'went': 1, 'huge': 1, 'mountain': 1, '픽사': 1, '베이': 1, '통통': 1, '몸집': 1, '어울': 1, '리지': 1, '주식': 1, '독특': 1, '식성': 1, '최소': 1, '년터': 1, '가능성': 1, '현지': 1, '외신': 1, '미국': 1, '로스앤젤레스': 1, '자연사': 1, '박물관': 1, '왕샤오밍': 1, '분석': 1, '존재': 1, '중국': 1, '남부': 1, '윈난성': 1, '자오퉁시': 1, '슈이탕바': 1, '지역': 1, '발굴': 1, '고대': 1, '손목': 1, '부위': 1, '돌출': 1, '진번': 1, '손가락': 1, '일명': 1, '뉴시스': 1, '도록': 1, '초식동물': 1, '추적': 1, '역할': 1, '전문가': 1, '보고': 1, '그동안': 1, '적적': 1, '전인': 1, '비교': 1, '진여': 1, '무려': 1, '년거슬러올': 1, '간다': 1, '증명': 1, '현대': 1, '길이': 1, '직선': 1, '모양': 1, '잡고': 1, '먹이': 1, '몸무게': 1, '지탱': 1, '용도': 1, '과정': 1, '갈고리': 1, '고분': 1, '먹기': 1, '위해': 1, '줄기': 1, '중요': 1, '적응': 1, '라며': 1, '육식성': 1, '화해': 1, '은장': 1, '설명': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"판다\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('판다', 9), ('대나무', 7), ('엄지', 7), ('가짜', 6), ('연구', 5), ('진화', 4), ('만년', 3), ('화석', 3), ('결과', 2), ('박사', 2)]\n",
      "{'판다': 1, '대나무': 2, '엄지': 3, '가짜': 4, '연구': 5, '진화': 6, '만년': 7, '화석': 8, '결과': 9, '박사': 10}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)\n",
    "\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "    \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 1, '대나무': 2, '엄지': 3, '가짜': 4, '연구': 5, '진화': 6, '만년': 7, '화석': 8, '결과': 9, '박사': 10, '최근': 11, '자이언트판다': 12, '조상': 13, '일루': 14, '락토스': 15, '데쓰': 16, '발견': 17, '가장': 18, 'barber': 19, 'went': 20, 'huge': 21, 'mountain': 22, '픽사': 23, '베이': 24, '통통': 25, '몸집': 26, '어울': 27, '리지': 28, '주식': 29, '독특': 30, '식성': 31, '최소': 32, '년터': 33, '가능성': 34, '현지': 35, '외신': 36, '미국': 37, '로스앤젤레스': 38, '자연사': 39, '박물관': 40, '왕샤오밍': 41, '분석': 42, '존재': 43, '중국': 44, '남부': 45, '윈난성': 46, '자오퉁시': 47, '슈이탕바': 48, '지역': 49, '발굴': 50, '고대': 51, '손목': 52, '부위': 53, '돌출': 54, '진번': 55, '손가락': 56, '일명': 57, '뉴시스': 58, '도록': 59, '초식동물': 60, '추적': 61, '역할': 62, '전문가': 63, '보고': 64, '그동안': 65, '적적': 66, '전인': 67, '비교': 68, '진여': 69, '무려': 70, '년거슬러올': 71, '간다': 72, '증명': 73, '현대': 74, '길이': 75, '직선': 76, '모양': 77, '잡고': 78, '먹이': 79, '몸무게': 80, '지탱': 81, '용도': 82, '과정': 83, '갈고리': 84, '고분': 85, '먹기': 86, '위해': 87, '줄기': 88, '중요': 89, '적응': 90, '라며': 91, '육식성': 92, '화해': 93, '은장': 94, '설명': 95}\n",
      "\n",
      "OrderedDict([('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('판다', 9), ('픽사', 1), ('베이', 1), ('통통', 1), ('몸집', 1), ('어울', 1), ('리지', 1), ('대나무', 7), ('주식', 1), ('독특', 1), ('식성', 1), ('최소', 1), ('년터', 1), ('가능성', 1), ('연구', 5), ('결과', 2), ('현지', 1), ('외신', 1), ('미국', 1), ('로스앤젤레스', 1), ('자연사', 1), ('박물관', 1), ('왕샤오밍', 1), ('박사', 2), ('최근', 2), ('만년', 3), ('자이언트판다', 2), ('조상', 2), ('일루', 2), ('락토스', 2), ('화석', 3), ('분석', 1), ('데쓰', 2), ('가짜', 6), ('엄지', 7), ('존재', 1), ('중국', 1), ('남부', 1), ('윈난성', 1), ('자오퉁시', 1), ('슈이탕바', 1), ('지역', 1), ('발굴', 1), ('고대', 1), ('손목', 1), ('부위', 1), ('발견', 2), ('돌출', 1), ('진번', 1), ('손가락', 1), ('일명', 1), ('뉴시스', 1), ('도록', 1), ('초식동물', 1), ('진화', 4), ('추적', 1), ('역할', 1), ('전문가', 1), ('보고', 1), ('그동안', 1), ('적적', 1), ('전인', 1), ('비교', 1), ('진여', 1), ('무려', 1), ('년거슬러올', 1), ('간다', 1), ('증명', 1), ('현대', 1), ('길이', 1), ('직선', 1), ('모양', 1), ('잡고', 1), ('먹이', 1), ('몸무게', 1), ('지탱', 1), ('용도', 1), ('과정', 1), ('갈고리', 1), ('고분', 1), ('먹기', 1), ('위해', 1), ('줄기', 1), ('가장', 2), ('중요', 1), ('적응', 1), ('라며', 1), ('육식성', 1), ('화해', 1), ('은장', 1), ('설명', 1)])\n",
      "\n",
      "[[19, 20, 21, 22, 1, 23, 24, 25, 26, 27, 28, 2, 29, 1, 1, 30, 31, 32, 33, 34, 5, 9, 35, 36, 37, 38, 39, 40, 41, 10, 5, 11, 7, 12, 13, 14, 15, 8, 42, 9, 2, 16, 4, 3, 43, 8, 44, 45, 46, 47, 48, 49, 50, 7, 51, 1, 14, 15, 52, 53, 17, 54, 5, 12, 55, 56, 57, 4, 3, 1, 4, 3, 58, 1, 3, 2, 59, 16, 1, 60, 6, 61, 62, 63, 64, 65, 6, 66, 7, 67, 68, 11, 69, 5, 70, 71, 72, 73, 8, 17, 4, 3, 74, 1, 75, 76, 77, 5, 4, 3, 2, 78, 79, 80, 81, 82, 83, 84, 6, 85, 10, 2, 86, 87, 88, 2, 18, 89, 90, 91, 92, 13, 93, 2, 1, 94, 4, 3, 18, 6, 95]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 1, '대나무': 2, '엄지': 3, '가짜': 4, '연구': 5, '진화': 6, '만년': 7, '화석': 8, '결과': 9, '박사': 10, '최근': 11, '자이언트판다': 12, '조상': 13, '일루': 14, '락토스': 15, '데쓰': 16, '발견': 17, '가장': 18, 'barber': 19, 'went': 20, 'huge': 21, 'mountain': 22, '픽사': 23, '베이': 24, '통통': 25, '몸집': 26, '어울': 27, '리지': 28, '주식': 29, '독특': 30, '식성': 31, '최소': 32, '년터': 33, '가능성': 34, '현지': 35, '외신': 36, '미국': 37, '로스앤젤레스': 38, '자연사': 39, '박물관': 40, '왕샤오밍': 41, '분석': 42, '존재': 43, '중국': 44, '남부': 45, '윈난성': 46, '자오퉁시': 47, '슈이탕바': 48, '지역': 49, '발굴': 50, '고대': 51, '손목': 52, '부위': 53, '돌출': 54, '진번': 55, '손가락': 56, '일명': 57, '뉴시스': 58, '도록': 59, '초식동물': 60, '추적': 61, '역할': 62, '전문가': 63, '보고': 64, '그동안': 65, '적적': 66, '전인': 67, '비교': 68, '진여': 69, '무려': 70, '년거슬러올': 71, '간다': 72, '증명': 73, '현대': 74, '길이': 75, '직선': 76, '모양': 77, '잡고': 78, '먹이': 79, '몸무게': 80, '지탱': 81, '용도': 82, '과정': 83, '갈고리': 84, '고분': 85, '먹기': 86, '위해': 87, '줄기': 88, '중요': 89, '적응': 90, '라며': 91, '육식성': 92, '화해': 93, '은장': 94, '설명': 95}\n",
      "\n",
      "OrderedDict([('barber', 1), ('went', 1), ('huge', 1), ('mountain', 1), ('판다', 9), ('픽사', 1), ('베이', 1), ('통통', 1), ('몸집', 1), ('어울', 1), ('리지', 1), ('대나무', 7), ('주식', 1), ('독특', 1), ('식성', 1), ('최소', 1), ('년터', 1), ('가능성', 1), ('연구', 5), ('결과', 2), ('현지', 1), ('외신', 1), ('미국', 1), ('로스앤젤레스', 1), ('자연사', 1), ('박물관', 1), ('왕샤오밍', 1), ('박사', 2), ('최근', 2), ('만년', 3), ('자이언트판다', 2), ('조상', 2), ('일루', 2), ('락토스', 2), ('화석', 3), ('분석', 1), ('데쓰', 2), ('가짜', 6), ('엄지', 7), ('존재', 1), ('중국', 1), ('남부', 1), ('윈난성', 1), ('자오퉁시', 1), ('슈이탕바', 1), ('지역', 1), ('발굴', 1), ('고대', 1), ('손목', 1), ('부위', 1), ('발견', 2), ('돌출', 1), ('진번', 1), ('손가락', 1), ('일명', 1), ('뉴시스', 1), ('도록', 1), ('초식동물', 1), ('진화', 4), ('추적', 1), ('역할', 1), ('전문가', 1), ('보고', 1), ('그동안', 1), ('적적', 1), ('전인', 1), ('비교', 1), ('진여', 1), ('무려', 1), ('년거슬러올', 1), ('간다', 1), ('증명', 1), ('현대', 1), ('길이', 1), ('직선', 1), ('모양', 1), ('잡고', 1), ('먹이', 1), ('몸무게', 1), ('지탱', 1), ('용도', 1), ('과정', 1), ('갈고리', 1), ('고분', 1), ('먹기', 1), ('위해', 1), ('줄기', 1), ('가장', 2), ('중요', 1), ('적응', 1), ('라며', 1), ('육식성', 1), ('화해', 1), ('은장', 1), ('설명', 1)])\n",
      "\n",
      "[[1, 2, 1, 1, 5, 9, 10, 5, 7, 8, 9, 2, 4, 3, 8, 7, 1, 5, 4, 3, 1, 4, 3, 1, 3, 2, 1, 6, 6, 7, 5, 8, 4, 3, 1, 5, 4, 3, 2, 6, 10, 2, 2, 2, 1, 4, 3, 6]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 10개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
    "print()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 1, '대나무': 2, '엄지': 3, '가짜': 4, '연구': 5, '진화': 6, '만년': 7, '화석': 8, '결과': 9, '박사': 10}\n",
      "OrderedDict([('판다', 9), ('대나무', 7), ('연구', 5), ('결과', 2), ('박사', 2), ('만년', 3), ('화석', 3), ('가짜', 6), ('엄지', 7), ('진화', 4)])\n",
      "[[1, 2, 1, 1, 5, 9, 10, 5, 7, 8, 9, 2, 4, 3, 8, 7, 1, 5, 4, 3, 1, 4, 3, 1, 3, 2, 1, 6, 6, 7, 5, 8, 4, 3, 1, 5, 4, 3, 2, 6, 10, 2, 2, 2, 1, 4, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for word in words_frequency:\n",
    "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n",
      "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from one_encoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '는', '언제', '부터', '대나무', '만', '먹었을까', '?', '600', '만년', '전', '화석', '봤', '더니', '…']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens = twitter.morphs(chosun_eilbo_database['title'][0])\n",
    "print(tokens)\n",
    "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
    "for i in tokens:\n",
    "    print(one_hot_encoding(i, word_to_index=word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "7\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "from padding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  1  8  5]\n",
      " [ 0  0  0  0  1  3  5]\n",
      " [ 0  0  0  0  0  9  2]\n",
      " [ 0  0  0  2  4  3  2]\n",
      " [ 0  0  0  0  0  3  2]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  2]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 0  0  0  1 12  3 13]]\n"
     ]
    }
   ],
   "source": [
    "padded_1 = pad_sequences(encoded)\n",
    "print(padded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_2 = pad_sequences(encoded, padding='post')\n",
    "padded_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n",
      "\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n",
      "\n",
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 7  7  3  2 10]\n",
      " [ 1 12  3 13  0]]\n",
      "\n",
      "14\n",
      "\n",
      "[[ 1  5 14 14 14 14 14]\n",
      " [ 1  8  5 14 14 14 14]\n",
      " [ 1  3  5 14 14 14 14]\n",
      " [ 9  2 14 14 14 14 14]\n",
      " [ 2  4  3  2 14 14 14]\n",
      " [ 3  2 14 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  2 14 14 14 14]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13 14 14 14]]\n"
     ]
    }
   ],
   "source": [
    "padded_3 = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "print(padded_3)\n",
    "print()\n",
    "padded_4 = pad_sequences(encoded, padding='post', truncating='post')\n",
    "print(padded_4)\n",
    "print()\n",
    "padded_5 = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "print(padded_5)\n",
    "print()\n",
    "# 단어 집합의 크기보다 1 큰 숫자를 사용\n",
    "last_value = len(tokenizer.word_index) + 1\n",
    "print(last_value)\n",
    "print()\n",
    "padded_6 = pad_sequences(encoded, padding='post', value=last_value)\n",
    "print(padded_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF를 이용하여 벡터화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag_of_words 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n",
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\n",
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n",
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n",
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n",
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n",
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from bag_of_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'판다.픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다. 판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다. 일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과, 대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다. 화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다.약 만년 전 살았던 고대 판다일루락토스이다. 눈여겨볼은 손목 부위 발견된 돌출뼈다. 연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다가짜 엄지.뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다. 판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고. 그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데, 연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다. 화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다.연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데, 과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다. 왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(regular_expression_sentence) \n",
    "\n",
    "kospacing_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다': 0, '픽사': 1, '베이': 2, '통통한': 3, '몸집': 4, '어울리지': 5, '않게': 6, '대나무': 7, '주식': 8, '하는': 9, '독특한': 10, '식성': 11, '최소': 12, '만년': 13, '터': 14, '이어졌을': 15, '가능성': 16, '있다는': 17, '연구': 18, '결과': 19, '나왔다': 20, '일': 21, '현지': 22, '외신': 23, '따르면': 24, '미국': 25, '로스앤젤레스': 26, '자연사': 27, '박물관': 28, '왕샤오밍': 29, '박사': 30, '연': 31, '구': 32, '팀': 33, '은': 34, '최근': 35, '약': 36, '전': 37, '자이언트': 38, '조상': 39, '인': 40, '일루': 41, '락토스': 42, '화': 43, '석': 44, '분석': 45, '한': 46, ',': 47, '잡는데': 48, '쓰이는': 49, '‘': 50, '가짜': 51, '엄지': 52, '’': 53, '존재': 54, '했다고': 55, '밝혔다': 56, '화석': 57, '중국': 58, '남부': 59, '윈난성': 60, '자오퉁시': 61, '슈이': 62, '탕바': 63, '지역': 64, '발굴': 65, '됐다': 66, '살았던': 67, '고대': 68, '이다': 69, '눈': 70, '여겨': 71, '볼': 72, '손목': 73, '부위': 74, '발견': 75, '된': 76, '돌출': 77, '뼈': 78, '다': 79, '가': 80, '진번': 81, '째': 82, '손가락': 83, '일명': 84, '라고': 85, '말': 86, '했다': 87, '뉴시스': 88, '는': 89, '보다': 90, '쉽게': 91, '잡수': 92, '있도록': 93, '데': 94, '쓰인다': 95, '초식': 96, '동물': 97, '진화': 98, '하는데': 99, '중추': 100, '적': 101, '역할': 102, '했을': 103, '전문가': 104, '들': 105, '보고': 106, '그동안': 107, '같은': 108, '적응': 109, '비교': 110, '이뤄진': 111, '여겨져': 112, '왔는데': 113, '무려': 114, '거슬러': 115, '올라간다는': 116, '증명': 117, '해낸': 118, '셈': 119, '현': 120, '대': 121, '더': 122, '긴': 123, '길이': 124, '직선': 125, '모양': 126, '하고': 127, '있었다': 128, '“': 129, '잡고': 130, '뜯어': 131, '먹을는': 132, '먹이': 133, '찾아': 134, '걸어갈': 135, '몸무게': 136, '지탱': 137, '용도': 138, '도': 139, '쓰이는데': 140, '과정': 141, '짧은': 142, '갈고리': 143, '형': 144, '하게': 145, '”': 146, '이라고': 147, '왕': 148, '먹기': 149, '좋게': 150, '쪼개기': 151, '위해': 152, '줄기': 153, '단단히': 154, '붙잡는': 155, '많은': 156, '양대': 157, '나무': 158, '먹는데': 159, '가장': 160, '중요한': 161, '라며': 162, '육식성': 163, '해': 164, '만': 165, '먹는': 166, '종': 167, '바뀐': 168, '장애': 169, '넘어야': 170, '이고': 171, '그': 172, '중': 173, '놀라운': 174, '설명': 175}\n",
      "[11, 1, 1, 1, 1, 1, 1, 6, 1, 3, 1, 1, 1, 5, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 8, 2, 3, 3, 2, 2, 3, 2, 2, 1, 1, 2, 1, 3, 1, 1, 2, 7, 7, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 6, 2, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab, bow = build_bag_of_words(kospacing_sent)\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 9\n",
      "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']\n",
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n",
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from tf_idf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판다.픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다. 판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다. 일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과, 대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다. 화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다.약 만년 전 살았던 고대 판다일루락토스이다. 눈여겨볼은 손목 부위 발견된 돌출뼈다. 연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다. 판다가짜 엄지.뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다. 판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고. 그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데, 연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다. 화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다.연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데, 과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다. 왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다.\n"
     ]
    }
   ],
   "source": [
    "kospacing_sent \n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판다', '픽사 베이통통한 몸집 어울리지 않게 대나무 주식하는 판다', ' 판다독특한 식성 최소 만년 터이어졌을 가능성 있다는 연구 결과 나왔다', ' 일 현지 외신 따르면 미국 로스앤젤레스 자연사박물관 왕샤오밍 박사 연구팀은 최근 약 만년 전자이언트 판다 조상인 일루락토스 화 석 분석한 결과', ' 대나무 잡는데 쓰이는 ‘가짜 엄지’ 존재했다고 밝혔다', ' 화석은 중국 남부 윈난성 자오퉁시 슈이 탕바지역 발굴됐다', '약 만년 전 살았던 고대 판다일루락토스이다', ' 눈여겨볼은 손목 부위 발견된 돌출뼈다', ' 연구팀은 뼈 자이언트 판다가 진번째 손가락 일명 ‘가짜 엄지’라고 말했다', ' 판다가짜 엄지', '뉴시스판다가짜 엄지는 대나무보다 쉽게 잡수 있도록 하는 데 쓰인다', ' 판다초식 동물 진화하는데 중추적인 역할했을 전문가들은 보고', ' 그동안 같은 진화적 적응은 약 만년 전 인 비교적 최근 이뤄진 여겨져 왔는데', ' 연구 무려 만년 거슬러 올라간다는 증명해낸 셈이다', ' 화석 발견된 가짜 엄지는 현대판다보다 더 긴 길이 직선 모양하고 있었다', '연구팀은 “가짜 엄지는 대나무 잡고 뜯어먹을는 먹이 찾아 걸어갈 몸무게 지탱하는 용도도 쓰이는데', ' 과정 긴 뼈 짧은 갈고리형 진화하게 된”이라고 분석했다', ' 왕 박사는 “대나무 먹기 좋게 쪼개기 위해 줄기 단단히 붙잡는 은 많은 양대나무 먹는데 가장 중요한 적응”라며 “육식성 조상 진화해대나무만 먹는 종 바뀐 판다는 많은 장애 넘어야 했을 이고 가짜 엄지는 그 중 가장 놀라운 진화”라고 설명했다', '']\n"
     ]
    }
   ],
   "source": [
    "kospacing_sent_list = []\n",
    "list1 = kospacing_sent.split('.')\n",
    "for i in list1:\n",
    "    i_list1 = i.split(',')\n",
    "    for i_2 in i_list1:\n",
    "        kospacing_sent_list.append(i_2)\n",
    "print(kospacing_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 112\n",
      "['가능성', '가장', '가짜', '간다', '갈고리', '거슬러', '결과', '고대', '과정', '그', '그동안', '길이', '남부', '눈', '뉴시스', '대나무', '더', '데', '도록', '독특', '돌출', '동물', '라며', '락토스', '로스앤젤레스', '리지', '만년', '말', '먹기', '먹이', '모양', '몸무게', '몸집', '무려', '미국', '바지', '박물관', '박사', '발견', '발굴', '베이', '보고', '볼', '부위', '분석', '비교', '뼈', '석', '설명', '셈', '손가락', '손목', '슈이', '식성', '약', '양', '어울', '엄지', '역', '역할', '연구', '왕', '왕샤오밍', '외신', '용도', '위해', '윈난성', '육식성', '은', '인', '일', '일루', '일명', '자연사', '자오퉁시', '자이언트', '잡고', '장애', '적', '적응', '전', '전문가', '조상', '존재', '종', '주식', '줄기', '중', '중국', '중요', '증명', '지탱', '직선', '진', '진번', '진화', '초식', '최근', '최소', '추적', '탕', '터', '통통', '팀', '판다', '픽사', '현대', '현지', '형', '화', '화석', '화해']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(word for kospacing_sent in kospacing_sent_list for word in twitter.nouns(kospacing_sent)))\n",
    "vocab.sort()\n",
    "print('단어장의 크기 :', len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "N = len(kospacing_sent_list)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_sent_list[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "        \n",
    "tf_ = pd.DataFrame(result, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능성</th>\n",
       "      <th>가장</th>\n",
       "      <th>가짜</th>\n",
       "      <th>간다</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>거슬러</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그</th>\n",
       "      <th>...</th>\n",
       "      <th>통통</th>\n",
       "      <th>팀</th>\n",
       "      <th>판다</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대</th>\n",
       "      <th>현지</th>\n",
       "      <th>형</th>\n",
       "      <th>화</th>\n",
       "      <th>화석</th>\n",
       "      <th>화해</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    가능성  가장  가짜  간다  갈고리  거슬러  결과  고대  과정  그  ...  통통  팀  판다  픽사  현대  현지  형  \\\n",
       "0     0   0   0   0    0    0   0   0   0  0  ...   0  0   1   0   0   0  0   \n",
       "1     0   0   0   0    0    0   0   0   0  0  ...   1  0   1   1   0   0  0   \n",
       "2     1   0   0   0    0    0   1   0   0  0  ...   0  0   1   0   0   0  0   \n",
       "3     0   0   0   0    0    0   1   0   0  0  ...   0  1   1   0   0   1  0   \n",
       "4     0   0   1   0    0    0   0   0   0  0  ...   0  0   0   0   0   0  0   \n",
       "5     0   0   0   0    0    0   0   0   0  0  ...   0  0   0   0   0   0  0   \n",
       "6     0   0   0   0    0    0   0   1   0  0  ...   0  0   1   0   0   0  0   \n",
       "7     0   0   0   0    0    0   0   0   0  0  ...   0  0   0   0   0   0  0   \n",
       "8     0   0   1   0    0    0   0   0   0  0  ...   0  1   1   0   0   0  0   \n",
       "9     0   0   1   0    0    0   0   0   0  0  ...   0  0   1   0   0   0  0   \n",
       "10    0   0   1   0    0    0   0   0   0  0  ...   0  0   1   0   0   0  0   \n",
       "11    0   0   0   0    0    0   0   0   0  0  ...   0  0   1   0   0   0  0   \n",
       "12    0   0   0   0    0    0   0   0   0  1  ...   0  0   0   0   0   0  0   \n",
       "13    0   0   0   1    0    1   0   0   0  0  ...   0  0   0   0   0   0  0   \n",
       "14    0   0   1   0    0    0   0   0   0  0  ...   0  0   1   0   1   0  0   \n",
       "15    0   0   1   0    0    0   0   0   0  0  ...   0  1   0   0   0   0  0   \n",
       "16    0   0   0   0    1    0   0   0   1  0  ...   0  0   0   0   0   0  1   \n",
       "17    0   2   1   0    0    0   0   0   0  1  ...   0  0   1   0   0   0  0   \n",
       "18    0   0   0   0    0    0   0   0   0  0  ...   0  0   0   0   0   0  0   \n",
       "\n",
       "    화  화석  화해  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   1   0   0  \n",
       "4   0   0   0  \n",
       "5   1   1   0  \n",
       "6   0   0   0  \n",
       "7   0   0   0  \n",
       "8   0   0   0  \n",
       "9   0   0   0  \n",
       "10  0   0   0  \n",
       "11  1   0   0  \n",
       "12  1   0   0  \n",
       "13  0   0   0  \n",
       "14  1   1   0  \n",
       "15  0   0   0  \n",
       "16  1   0   0  \n",
       "17  2   0   1  \n",
       "18  0   0   0  \n",
       "\n",
       "[19 rows x 112 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>가능성</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>가장</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>가짜</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>간다</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>갈고리</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>현지</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>형</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>화</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>화석</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>화해</th>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "가능성  1.386294\n",
       "가장   1.386294\n",
       "가짜   1.386294\n",
       "간다   1.386294\n",
       "갈고리  1.386294\n",
       "..        ...\n",
       "현지   1.386294\n",
       "형    1.386294\n",
       "화    1.386294\n",
       "화석   1.386294\n",
       "화해   1.386294\n",
       "\n",
       "[112 rows x 1 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가능성</th>\n",
       "      <th>가장</th>\n",
       "      <th>가짜</th>\n",
       "      <th>간다</th>\n",
       "      <th>갈고리</th>\n",
       "      <th>거슬러</th>\n",
       "      <th>결과</th>\n",
       "      <th>고대</th>\n",
       "      <th>과정</th>\n",
       "      <th>그</th>\n",
       "      <th>...</th>\n",
       "      <th>통통</th>\n",
       "      <th>팀</th>\n",
       "      <th>판다</th>\n",
       "      <th>픽사</th>\n",
       "      <th>현대</th>\n",
       "      <th>현지</th>\n",
       "      <th>형</th>\n",
       "      <th>화</th>\n",
       "      <th>화석</th>\n",
       "      <th>화해</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         가능성        가장        가짜        간다       갈고리       거슬러        결과  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   1.386294  0.000000  0.000000  0.000000  0.000000  0.000000  1.386294   \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.386294   \n",
       "4   0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  1.386294  0.000000  1.386294  0.000000   \n",
       "14  0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  1.386294  0.000000  0.000000   \n",
       "17  0.000000  2.772589  1.386294  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          고대        과정         그  ...        통통         팀        판다        픽사  \\\n",
       "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "1   0.000000  0.000000  0.000000  ...  1.386294  0.000000  1.386294  1.386294   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  1.386294  1.386294  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   1.386294  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.000000  1.386294  1.386294  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "12  0.000000  0.000000  1.386294  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  1.386294  0.000000  0.000000   \n",
       "16  0.000000  1.386294  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  1.386294  ...  0.000000  0.000000  1.386294  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          현대        현지         형         화        화석        화해  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  1.386294  0.000000  1.386294  0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000  0.000000  1.386294  1.386294  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  1.386294  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  1.386294  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14  1.386294  0.000000  0.000000  1.386294  1.386294  0.000000  \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16  0.000000  0.000000  1.386294  1.386294  0.000000  0.000000  \n",
       "17  0.000000  0.000000  0.000000  2.772589  0.000000  1.386294  \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[19 rows x 112 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = kospacing_sent_list[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
